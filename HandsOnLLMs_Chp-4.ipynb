{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18018e6a-7140-4cb7-8d5b-0670d083480c",
   "metadata": {},
   "source": [
    "## Text Classification with Representation Model\n",
    "A foundation model is fine-tuned for specific tasks; for instance, to perform classification or generate general-purpose embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e57164d-af67-4694-afbe-00809c75ff45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c44228-2d8f-41f1-bea7-9967eee00b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c095efe-d10d-491f-8702-5d1d6ceecb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "y_pred = []\n",
    "for output in tqdm(pipe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c4961-1e22-410c-a2bc-c9c5196cdca7",
   "metadata": {},
   "source": [
    "## The Sentiment of Movie Review Dataset\n",
    "To load this data, we make use of the datasets package, which will be used throughout the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2f203-81c7-48a0-8a82-191986639d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load our data\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38bdfe9-4c6b-4938-aa79-bfb5a6b3fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b822ee92-6ae9-4cc2-845b-7e5e584cfc04",
   "metadata": {},
   "source": [
    "## Using a Task Specific Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe79344-45f1-4219-addd-c35e04cdce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d14be3d-a419-4142-aa8e-a8e415218c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Path to our HF Model\n",
    "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "# load the model into pipeline\n",
    "pipe = pipeline(\n",
    "    model = model_path,\n",
    "    tokenizer = model_path,\n",
    "    return_all_scores = True,\n",
    "    device = \"mps\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f55618a-ed85-4d7a-a1bc-c0e6aec2a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02a1b8-1ba2-401a-9c8a-33c06018cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Inference\n",
    "y_pred = []\n",
    "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")),\n",
    "                  total = len(data[\"test\"])\n",
    "                  ):\n",
    "    negative_scores = output[0]['score']\n",
    "    positive_scores = output[2]['score']\n",
    "    assignment = np.argmax([negative_scores, positive_scores])\n",
    "    y_pred.append(assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b21d43-3dc5-4dee-8ac8-8aaf5ca9a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3265ac-98aa-4ce6-893e-ebe4a6f11425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(y_true, y_pred):\n",
    "    \"\"\"Create and print the classification report\"\"\"\n",
    "    performance = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names = [\"Negative Review\", \"Positive Review\"]\n",
    "    )\n",
    "    print(performance)\n",
    "\n",
    "\n",
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f9c23-84fa-4aef-964c-d26c069ae0c1",
   "metadata": {},
   "source": [
    "### Supervised Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b2fef-60f6-4f95-978a-af0d869dc0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b72d08-a194-4e8b-8145-2fde3d12eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Convert the text to embeddings\n",
    "train_embeddings = model.encode(data[\"train\"][\"text\"], show_progress_bar = True)\n",
    "test_embeddings = model.encode(data[\"test\"][\"text\"], show_progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f516afc-091b-409d-9ad4-1be5ab2887c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_embeddings.shape)\n",
    "print(test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78038d5c-7641-4ee1-ba74-5e61a001dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66116e61-ebab-4204-9097-d2b69fc028ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression on out train embeddings\n",
    "clf = LogisticRegression(random_state = 42)\n",
    "clf.fit(train_embeddings, data[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d4b40-8815-4072-9f7d-4fa667d07d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating our model\n",
    "y_pred = clf.predict(test_embeddings)\n",
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce6150-a73f-45cb-86c7-31e770857bc7",
   "metadata": {},
   "source": [
    "### What if we do not have labeled data\n",
    "\n",
    "Zero-shot classification attempts to predict the labels of input text even though it was not trained on them. In zero-shot classification, we have no labeled data, only the labels themselves. The zero-shot model decides how the input is related to the candidate labels.\n",
    "\n",
    "To embed the labels, we first need to give them a description, such as “a\n",
    "negative movie review.” This can then be embedded through sentence-transformers.\n",
    "\n",
    "We can create these label embeddings using the `.encode` function.\n",
    "\n",
    "The cosine similarity is the angle between two vectors or embeddings. In this\n",
    "example, we calculate the similarity between a document and the two possible labels, positive and negative.\n",
    "\n",
    "After embedding the label descriptions and the documents, we can use\n",
    "cosine similarity for each label document pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021385e-1c4b-4e24-b987-5126c83f62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for out labels\n",
    "label_embeddings = model.encode([\"A very negative movie review\", \"A very positive movie review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d09ed5-64b7-4ef7-b9dc-90f0746978f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Find the best matching label for each document\n",
    "sim_matrix = cosine_similarity(test_embeddings, label_embeddings)\n",
    "y_pred = np.argmax(sim_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d9146-73b3-41f5-b4fd-75dff73af87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4769ebc8-73ba-4269-acb8-4e1c0ba95656",
   "metadata": {},
   "source": [
    "### Text Classification with Generative Models\n",
    "\n",
    "- sequence to sequence models (take as input some text and generative text)\n",
    "\n",
    "A task-specific model generates numerical values from sequences of tokens\n",
    "while a generative model generates sequences of tokens from sequences of tokens.\n",
    "\n",
    "These generative models are generally trained on a wide variety of tasks and usually do not perform your use case out of the box. For instance, if we give a generative model a movie review without any context, it has no idea what to do with it.\n",
    "\n",
    "Instead, we need to help it understand the context and guide it toward the answers that we are looking for. As demonstrated in Figure 4-18, this guiding process is done mainly through the instruction, or prompt, that you give such a model. Iteratively improving your prompt to get your preferred output is called prompt engineering.\n",
    "\n",
    "Prompt engineering allows prompts to be updated to improve the output\n",
    "generated by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0b4a0-7665-4487-924d-d7251de50edc",
   "metadata": {},
   "source": [
    "### Using Text to Text Transfer Transformer\n",
    "\n",
    "Like the decoder-only models, the encoder-decoder models are sequence-to-sequence models and generally fall in the category of generative models.\n",
    "\n",
    "An interesting family of models that leverage this architecture is the Text-to-Text Transfer Transformer or T5 model. Its architecture is similar to the original Transformer where 12 decoders and 12 encoders are stacked together.\n",
    "\n",
    "These models were first pretrained using masked language modeling. \n",
    "\n",
    "In the first step of training, namely pretraining, the T5 model needs to\n",
    "predict masks that could contain multiple tokens.\n",
    "\n",
    "The second step of training, namely fine-tuning the base model, is where the real magic happens. Instead of fine-tuning the model for one specific task, each task is converted to a sequence-to-sequence task and trained simultaneously. This allows the model to be trained on a wide variety of tasks.\n",
    "\n",
    "By converting specific tasks to textual instructions, the T5 model can be\n",
    "trained on a variety of tasks during fine-tuning.\n",
    "\n",
    "To use this pretrained Flan-T5 model for classification, we will start by loading it through the \"text2text-generation\" task, which is generally reserved for these encoder-decoder models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378d03f-5b4a-4dba-9713-2594213fdce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our Model\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model = \"google/flan-t5-base\",\n",
    "    device = \"mps:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698e23e-3430-49f8-84bc-c41918a3eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare our data\n",
    "prompt = \"Is the following sentence positive or negative \"\n",
    "data = data.map(lambda example: {\"t5\": prompt + example['text']})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da8644-3e4b-464a-b8ea-cecd8e8ffdb6",
   "metadata": {},
   "source": [
    "After creating our updated data, we can run the pipeline similar to the task-specific example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7239cd-72ae-4979-8a4c-f6e1d6b40ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference\n",
    "y_pred = []\n",
    "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"t5\")), total = len(data[\"test\"])):\n",
    "    text = output[0][\"generated_text\"]\n",
    "    y_pred.append(0 if text == \"negative\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec88c9-436f-4c4f-9cd2-e87c5a2561fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d1513-a32c-4833-a5db-e88cf01579b8",
   "metadata": {},
   "source": [
    "### ChatGPT for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038650cd-d75c-40d1-9371-b0164bb4f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b810d-77f8-4d8c-bf37-1a43a0025822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Client\n",
    "client = openai.OpenAI(api_key = \"YOUR API KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d22d80-e0b2-450a-86ec-90f7487badd0",
   "metadata": {},
   "source": [
    "Using this client, we create the chatgpt_generation function, which allows us to generate some text based on a specific prompt, input document, and the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9423d6b7-3558-4ef2-be1f-30389fcbee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgpt_generation(prompt, document, model = \"gpt-3.5-turbo-0125\"):\n",
    "    \"\"\"Generate an output based on prompt and an input document\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\" : \"You are a helpful assistant\"\n",
    "        }, \n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt.replace(\"[DOCUMENT]\", document)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages = messages,\n",
    "        model = model,\n",
    "        temperature = 0\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aade15-8eb4-4a5c-b6a7-be7de60404dc",
   "metadata": {},
   "source": [
    "We will need to create a template to ask the model to perform the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97508fe5-548e-48bd-8e7a-a4a9467d84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template as a base\n",
    "prompt = \"\"\"Predict whether the following document is a positive or negative movie review:\n",
    "\n",
    "[DOCUMENT]\n",
    "\n",
    "If it is a postive return 1 and if it is negative return 0. Do not give any other answers.\"\"\"\n",
    "\n",
    "# Predict the target using GPT\n",
    "document = \"unpretentious, charming, quirky, original\"\n",
    "chatgpt_generation(prompt, document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca21c994-9f65-47e0-be9a-9123ba739037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we can run this for all reviews in the test dataset to get its predictions.\n",
    "\n",
    "predictions = [\n",
    "    chatgpt_generation(prompt, doc) for doc in tqdm(data[\"test\"][\"text\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe3be9-2a56-4fe6-8e8d-913b1dbd299e",
   "metadata": {},
   "source": [
    "Like the previous example, we need to convert the output from strings to integers to evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311cb2b-7b39-4a20-8a50-b29ee17c978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions\n",
    "y_pred = [int(pred) for pred in predictions]\n",
    "\n",
    "# Evaluate performance\n",
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc6e78-2e1c-4c48-aa43-fc84e37925d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HandsOnLLMs",
   "language": "python",
   "name": "handsonllms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
