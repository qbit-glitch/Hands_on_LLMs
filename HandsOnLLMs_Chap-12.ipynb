{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Generation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Tuning with QLoRA\n",
    "\n",
    "### Templating Instruction Data\n",
    "\n",
    "Model - TinyLlama\n",
    "\n",
    "Dataset - UltraChat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a tokenizer to use it's char template\n",
    "template_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1BChat-v1.0\")\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format the prompt to using the <|user|> template TinyLlama is using.\"\"\"\n",
    "    # Format answers\n",
    "    chat = example[\"messages\"]\n",
    "    prompt = template_tokenizer.apply_chat_template(chat, tokenize = False)\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "\n",
    "# Load and format the data using the template TinyLlama is using\n",
    "dataset = (\n",
    "    load_dataset(\"HuggingFaceH4/ultrachat_200k\", split = \"test_sft\")\n",
    "        .shuffle(seed = 42)\n",
    "        .select(range(3_000))\n",
    ")\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of formatted prompt\n",
    "print(dataset[\"text\"][2576])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Quantization\n",
    "\n",
    "This is where we apply the Q in QLoRA, namely  quantization. We use the bitsandbytes package to compress the pretrained model to a 4-bit representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-1431k-3T\"\n",
    "\n",
    "# 4 bit quantization configuration - Q in QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,     # Use 4-bit precision model loading\n",
    "    bnb_4bit_quant_type = \"nf4\",    # Quantization Type\n",
    "    bnb_4bit_compute_dtype = \"float16\",      # Compute dtype\n",
    "    bnb_4bit_use_double_quant = True      # Apply nested quantization\n",
    ")\n",
    "\n",
    "# Load the model to train on the GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "model.config.use_cache = False,\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# Prepare LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha = 32,       # LoRA Scaling\n",
    "    lora_dropout = 0.1,    # Dropout for LoRA Layers\n",
    "    r = 64,      # Rank\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules = [\n",
    "        \"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \"o_proj\", \"down_proj\"\n",
    "    ]       # Layers to target\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    optim = \"pages_adamw_32bit\",\n",
    "    learning_rate = 2e-4,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    num_train_epochs = 1,\n",
    "    logging_steps = 10,\n",
    "    fp16 = True,\n",
    "    gradient_checkpointing = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_arguments,\n",
    "    max_seq_length = 512\n",
    "\n",
    "    # Leave this out for regular SFT\n",
    "    peft_config = peft_config\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save QLoRA weights\n",
    "trainer.model.save_pretrained(\"TinyLlama-1.1B-qlora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Weights\n",
    "\n",
    "After we have trained our QLoRA weights, we still need to combine them with the original weights to use them. We reload the model in 16 bits, instead of the quantized 4 bits, to merge the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage = True,\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging the adapter with the base model, we can use it with the prompt template that we defined earler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use our predefined prompt template\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about Large Language Models. </s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# Run our instruction-tuned model\n",
    "pipe = pipeline(task = \"text-generation\", model = merged_model, tokenizer = tokenizer)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preference Tuning with DPO\n",
    "\n",
    "### Templating Alignment Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset - `argilla/distilabel-intel-orca-dpo-pairs`\n",
    "\n",
    "Model - `TinyLlama/TinyLlama-1.1B-Chat-v1.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format the prompt to using the <|user|> template TinyLlama is using\"\"\"\n",
    "\n",
    "    # Format answers\n",
    "    system = \"<|system|>\\n\" + example[\"system\"] + \"</s>\\n\"\n",
    "    prompt = \"<|user|>\\n\" + example[\"input\"] + \"</s>\\n<|assistant|>\\n\"\n",
    "    chosen = example[\"chosen\"] + \"</s>\\n\"\n",
    "    rejected = example[\"rejected\"] + \"</s>\\n\"\n",
    "\n",
    "    return {\n",
    "        \"prompt\": system + prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Apply formatting to the dataset and select relatively short answers\n",
    "dpo_dataset = load_dataset(\n",
    "    \"argilla/distilabel-intel-orca-dpo-pairs\", split = \"train\"\n",
    ")\n",
    "\n",
    "dpo_dataset = dpo_dataset.filter(\n",
    "    lambda r:\n",
    "    r[\"status\"] != \"tie\" and\n",
    "    r[\"chosen_score\"] >= 8 and\n",
    "    not r[\"in_gsm8k_train\"] \n",
    ")\n",
    "\n",
    "dpo_dataset = dpo_dataset.map(\n",
    "    format_prompt,\n",
    "    remove_columns = dpo_dataset.column_names\n",
    ")\n",
    "\n",
    "dpo_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "# 4-bit quantization configuration - Q in QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,        # Use 4-bit precision model loading\n",
    "    bnb_4bit_quant_type = \"nf4\",     # Quantization type\n",
    "    bnb_4bit_compute_dtype = \"float16\",       # Compute type\n",
    "    bnb_4bit_use_double_quant = True,       # Apply nested quantization\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage = True,\n",
    "    device_map = \"auto\",\n",
    "    quantization_config = bnb_config\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_upload()\n",
    "\n",
    "\n",
    "# Load Llama tokenizer\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the same LoRA configuration as before to perform DPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# Prepare LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha = 32,    # LoRA Scaling\n",
    "    lora_dropout = 0.1,       # Dropout for LoRA Layers\n",
    "    r = 54,     # Rank\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules = [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \"o_proj\", \"down_proj\"]\n",
    "                    # Layers to target\n",
    ")\n",
    "\n",
    "# Prepare the model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOConfig\n",
    "\n",
    "ouput_dir = \"./results\"\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = DPOConfig(\n",
    "    output_dir = output_dir,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    optim = \"pages_adamw_32bit\",\n",
    "    learning_rate = 1e-5,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    max_steps = 200,\n",
    "    logging_steps = 10,\n",
    "    fp16 = True,\n",
    "    gradient_checkpointing = True,\n",
    "    warmup_ratio = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now that we have prepare all our models and parameters, we can start fine-tuning our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOTrainer\n",
    "\n",
    "# Create a DPO trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    args = training_arguments,\n",
    "    train_dataset = dpo_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    peft_config = peft_config,\n",
    "    beta = 0.1,\n",
    "    max_prompt_length = 512,\n",
    "    max_length = 512\n",
    ")\n",
    "\n",
    "# Fine tune the model with DPO\n",
    "dpo_trainer.train()\n",
    "\n",
    "\n",
    "# Save adapter\n",
    "dpo_trainer.model.save_pretrained(\"TinyLlama-1.1B-dpo-qlora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Merge the LoRA and base model\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage = True,\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "sft_model = model.merge_and_unload()\n",
    "\n",
    "# Merge DPO LoRA and SFT model\n",
    "dpo_model = PeftModel.from_pretrained(\n",
    "    sft_model,\n",
    "    \"TinyLlama-1.1B-dpo-qlora\",\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "dpo_model = dpo_model.merge_and_unload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HandsOnLLMs",
   "language": "python",
   "name": "handsonllms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
