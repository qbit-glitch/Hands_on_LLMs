{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Text Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Contrastive Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset : GLUE [Task -> MNLI]\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNLI dataset from GLUE\n",
    "# 0 = entailment, 1 = neutral, 2 = contradiction\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"glue\", \"mnli\", split = \"train\"\n",
    ").select(range(50_000))\n",
    "train_dataset = train_dataset.remove_columns(\"idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using BERT base model as pretrained transformer that serves as embeddings individual words\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Use a base model\n",
    "embedding_model = SentenceTransformer('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function - Softmax\n",
    "# Using BERT base model as pretrained transformer that serves as embeddings individual words\n",
    "\n",
    "from sentence_transformers import losses\n",
    "\n",
    "# Define the loss function. In softmax loss we will also need to explicitly set the number of labels.\n",
    "train_loss = losses.SoftmaxLoss(\n",
    "    model = embedding_model,\n",
    "    sentence_embedding_dimension = embedding_model.get_sentence_embedding_dimension(),\n",
    "    num_labels = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluation using Semantic Textual Similarity Benchmark (STSB)\n",
    "# Process STSB data to make sure all values are between 0 and 1\n",
    "\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "# Create an embedding similarity evaluator for STSB\n",
    "val_sts = load_dataset(\"glue\", \"stsb\", split = \"validation\")\n",
    "evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1 = val_sts[\"sentence1\"],\n",
    "    sentences2 = val_sts[\"sentence2\"],\n",
    "    scores = [score/5 for score in val_sts[\"label\"]],\n",
    "    main_similarity = \"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "\n",
    "# defining the training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir = \"base_embedding_model\",\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 32,\n",
    "    warmup_steps = 100,\n",
    "    fp16 = False,\n",
    "    eval_steps = 100,\n",
    "    logging_steps = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined our data, embedding model, loss, and evaluator, we can\n",
    "# start training our model. We can do that using SentenceTransformerTrainer\n",
    "\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "\n",
    "# Train embedding model\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model = embedding_model,\n",
    "    args = args,\n",
    "    train_dataset = train_dataset,\n",
    "    loss = train_loss,\n",
    "    evaluator = evaluator\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our trained model\n",
    "evaluator(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Load MNLI dataset from GLUE\n",
    "# 0 - Entailment ; 1 -> Neutral ; 2 -> Contradiction\n",
    "train_dataset = load_dataset(\n",
    "    \"glue\", \"mnli\", split = \"train\"\n",
    ").select(range(50_000))\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(\"idx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Neutral/Contradiction) -> 0 ; (Entailment) -> 1;\n",
    "mapping = {2:0, 1:0, 0:1}\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": train_dataset[\"premise\"],\n",
    "    \"sentence2\" : train_dataset[\"hypothesis\"],\n",
    "    \"label\" : [float(mapping[label]) for label in train_dataset[\"label\"]]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our evaluator\n",
    "\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "# Create an embedding similarity evaluator for stsb\n",
    "val_sts = load_dataset(\"glue\", \"stsb\", split = \"validation\")\n",
    "evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1 = val_sts[\"sentence1\"],\n",
    "    sentences2 = val_sts[\"sentence2\"],\n",
    "    scores = [score/5 for score in val_sts[\"label\"]],\n",
    "    main_similarity = \"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b9cb1fb55f4008aae40d38c29805b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527d721a7523406bb6cd555dd2792816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2306, 'grad_norm': 1.4475200176239014, 'learning_rate': 5e-05, 'epoch': 0.06}\n",
      "{'loss': 0.1707, 'grad_norm': 1.3311426639556885, 'learning_rate': 4.6582365003417636e-05, 'epoch': 0.13}\n",
      "{'loss': 0.1703, 'grad_norm': 1.24318265914917, 'learning_rate': 4.316473000683528e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1583, 'grad_norm': 1.0497990846633911, 'learning_rate': 3.9747095010252904e-05, 'epoch': 0.26}\n",
      "{'loss': 0.1518, 'grad_norm': 1.4255374670028687, 'learning_rate': 3.632946001367054e-05, 'epoch': 0.32}\n",
      "{'loss': 0.1571, 'grad_norm': 1.2196274995803833, 'learning_rate': 3.291182501708818e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1511, 'grad_norm': 1.109902262687683, 'learning_rate': 2.9494190020505813e-05, 'epoch': 0.45}\n",
      "{'loss': 0.1557, 'grad_norm': 1.4513516426086426, 'learning_rate': 2.6076555023923443e-05, 'epoch': 0.51}\n",
      "{'loss': 0.1478, 'grad_norm': 1.866289496421814, 'learning_rate': 2.2658920027341084e-05, 'epoch': 0.58}\n",
      "{'loss': 0.1461, 'grad_norm': 1.0433378219604492, 'learning_rate': 1.9241285030758715e-05, 'epoch': 0.64}\n",
      "{'loss': 0.149, 'grad_norm': 1.1681982278823853, 'learning_rate': 1.5823650034176352e-05, 'epoch': 0.7}\n",
      "{'loss': 0.1464, 'grad_norm': 1.5202412605285645, 'learning_rate': 1.2406015037593984e-05, 'epoch': 0.77}\n",
      "{'loss': 0.146, 'grad_norm': 1.5545415878295898, 'learning_rate': 8.988380041011621e-06, 'epoch': 0.83}\n",
      "{'loss': 0.1407, 'grad_norm': 1.3141279220581055, 'learning_rate': 5.570745044429255e-06, 'epoch': 0.9}\n",
      "{'loss': 0.1403, 'grad_norm': 1.1103897094726562, 'learning_rate': 2.15311004784689e-06, 'epoch': 0.96}\n",
      "{'train_runtime': 799.1686, 'train_samples_per_second': 62.565, 'train_steps_per_second': 1.956, 'train_loss': 0.1568457133024073, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1563, training_loss=0.1568457133024073, metrics={'train_runtime': 799.1686, 'train_samples_per_second': 62.565, 'train_steps_per_second': 1.956, 'total_flos': 0.0, 'train_loss': 0.1568457133024073, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a Cosine Similarity Loss Function\n",
    "\n",
    "from sentence_transformers import losses, SentenceTransformer\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "\n",
    "# Define model\n",
    "embedding_model = SentenceTransformer(\"bert-base-uncased\")\n",
    "\n",
    "# Loss Function\n",
    "train_loss = losses.CosineSimilarityLoss(model = embedding_model)\n",
    "\n",
    "# Define the training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir = \"cosineloss_embedding_model\",\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    warmup_steps = 100,\n",
    "    fp16 = False,\n",
    "    eval_steps = 100,\n",
    "    logging_steps = 100\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer =  SentenceTransformerTrainer(\n",
    "    model = embedding_model,\n",
    "    args = args,\n",
    "    train_dataset = train_dataset,\n",
    "    loss = train_loss,\n",
    "    evaluator = evaluator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pearson_cosine': 0.7228620455159176, 'spearman_cosine': 0.7249475950906865}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate our trained model\n",
    "evaluator(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Negative Ranking Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You lose the things to the following level if the people recall.',\n",
       " 'A member of my team will execute your orders with immense precision.',\n",
       " 'This information belongs to them.',\n",
       " \"I'm upset that my walkman broke and now I have to turn the stereo up really loud.\",\n",
       " \"Slate had an opinion on Jackson's findings.\",\n",
       " 'I burst through the doors and fell down.',\n",
       " 'Problems in data synthesis.',\n",
       " 'You can see that on television, as well.',\n",
       " 'We want to identify practices commonly used by agencies in the last 5 years',\n",
       " 'The other men were shuffled around.',\n",
       " 'It has been very intriguing.',\n",
       " 'He returned slowly to the bunkhouse.',\n",
       " 'The postal service could deliver less frequently.',\n",
       " 'Another shift in the economy was found to be more nutritious.',\n",
       " 'The rule has data collection requirements which aid the EPA to realize their emission control goals.',\n",
       " 'The remains of Voltaire and Rousseau, Hugo and Zola, assassinated Socialist leader Jean Jaurys, and Louis Braille are all interred in the crypt.',\n",
       " \"I can't remember their name\",\n",
       " \"Japan's new rulers were catching on quickly.\",\n",
       " 'These organizations invest lots of time to understand how some processes can contribute to or haampe',\n",
       " 'The facts are accessible to you.',\n",
       " 'It is the first set of production tooling for manufacturing.',\n",
       " 'The net present value of future additional resources for funding HI benefits was $4.',\n",
       " 'This is the only channel of expression of the advocacy that Congress seeks to restrict.',\n",
       " 'Can we fix this?',\n",
       " 'They had data that was very relevant but under used.',\n",
       " 'If individuals will invest their time, funding will come along, too.',\n",
       " 'We always stink.',\n",
       " \"The rule makes broadcasters keep a file about children's television programming.\",\n",
       " 'Citations from Marx were used in socialist papers in the 30s. ',\n",
       " \"He's not into this multivista stuff because what is some dumpy system compared to the magnificent smell of a cinnamon roll dunked in wholesome milk.\",\n",
       " 'Clark hoped that he could continue their marriage.',\n",
       " 'Gratianus was a Roman empoeror.',\n",
       " \"I went and told the truth at court but it didn't do me any good.\",\n",
       " 'Bye for now.',\n",
       " 'Can you tell me how to prove it?',\n",
       " 'The own hall meetings helped improve his customer satisfaction levels, said the executive.',\n",
       " 'New attorneys have massive law school debt.',\n",
       " 'One of the curiosities is a two-headed goat.',\n",
       " 'The group of guys he tends to hang out with gave him the nickname Jumper.',\n",
       " 'I think Las Vegas is the most boring place I know.',\n",
       " 'Please be quiet.',\n",
       " 'I will be able to make my own clothing.',\n",
       " 'The river is central to all vacations to Paris.',\n",
       " \"They're supposed to be bilingual \",\n",
       " 'It is over.',\n",
       " 'The knowledge captured would determine if the next step was further development or production.',\n",
       " 'My pride was foolish not well placed.',\n",
       " 'The girl spoke very quietly. ',\n",
       " 'There rules are not really clear so we ask our manager for help.',\n",
       " 'A Ku Klux Klan member has been elected in Louisiana.',\n",
       " 'They might at least try and do something about it first.',\n",
       " 'I am not a big fan of winter sports.',\n",
       " \"General education's focus is to develop students' critical thinking skills.\",\n",
       " 'Working with rigorous methodologists is important.',\n",
       " 'The new ones have things that can fuck with your mind.',\n",
       " 'The US paid more to the rest of the world than the rest of the world received in income.',\n",
       " 'Anything we should know?',\n",
       " 'There are races from April to December at Veliefendi Hippodrome.',\n",
       " 'Opinions tend to vary.',\n",
       " 'There is a seal to show authenticity. ',\n",
       " 'Dressing Jane will be enjoyable.',\n",
       " 'The influence of the following factors is possible.',\n",
       " 'Is their house near here?',\n",
       " 'Conservatives are battling now.',\n",
       " 'With the good road connections, both bays have been developed.',\n",
       " 'You may have to foot the bill if you order any service you want.',\n",
       " 'Sir Ernest bent his head slightly, and continued.',\n",
       " 'Hiding things is just dirty, whereas there is glory in fiction',\n",
       " 'Wall Street is facing issues, that need to be addressed. ',\n",
       " 'The New York Giants and Raiders are my favorite teams in football.',\n",
       " \"It wouldn't cost them much money to just stick on those things.\",\n",
       " 'Philip II of Spain invaded Portugal.',\n",
       " 'I am certainly in agreement with it.',\n",
       " 'I turned it over.',\n",
       " 'Lorenzo and Giuliano were related to one another.',\n",
       " 'The 10 branches had close to 100 employees.',\n",
       " 'The first two creditors divided their shares correctly.',\n",
       " 'There is little evidence left of that era.',\n",
       " 'Chief officers often spread their responsibility among senior-level managers. ',\n",
       " 'The clever Annie was not in proof.',\n",
       " 'Horus has a shrine.',\n",
       " 'Usps domestic rates for priority and first class mail ate described above. ',\n",
       " 'He call Francisco in Spanish.',\n",
       " 'Unless congress expresses an intent that such burdens be imposed, the Commission is unwilling to recommend an interpretation of the statute that produces such consequences.',\n",
       " 'The movie stars Matt Dillon and Gary Sinise.',\n",
       " 'probably yes so uh-huh',\n",
       " 'They bounce back from the bottom.',\n",
       " 'It is just a guess',\n",
       " 'The SPAN A guide is a guide used for legal service planning.',\n",
       " 'They made a good rendition.',\n",
       " 'The citizens of the U.S. are supposed to be proud of themselves.',\n",
       " 'She does, says, and wears everything perfectly, consistently. ',\n",
       " 'It is recommended that you leave river briefly before ending your trip at Montréal.',\n",
       " 'I think a good defense is the most important thing you need to win games.',\n",
       " 'It took twelve days to make another.',\n",
       " 'I like to get out in nature and de-stress.',\n",
       " '\"20 South Audley Mansions\" is a book that Mrs. Vandemeyer read.',\n",
       " 'The old ways of the native population has almost completely faded away.',\n",
       " 'Computer ownership by households usually increases in a year.',\n",
       " 'You and Tuppence have been hand in glove.',\n",
       " 'The mood changes below the Rond-Point, and a pleasant park leads you past two the Petit Palais, all steel and glass, and the Grand Palais.',\n",
       " 'Granting permission.',\n",
       " 'Yes, I heard that was really good.',\n",
       " 'The Black Eunuchs were responsible for the security and administration of the Harem.',\n",
       " 'Maybe he should head for California to clear out even farther?',\n",
       " 'During the summer months dealers barter for piles of silkworm cocoons.',\n",
       " 'Acid is handled by workers. ',\n",
       " 'Paris opened the first department store.',\n",
       " 'He was patient while hte group on the 18th hole finished.',\n",
       " 'He thought about ways to achieve his life goals for a long time.',\n",
       " 'The fireproof repository built in 1958 is considered to be enough to protect the artifacts.',\n",
       " 'The man was extremely large. ',\n",
       " \"The value of households' stock holdings dropped and this contributed to a decline in household wealth.\",\n",
       " 'Do you know where Tommy is?',\n",
       " 'Long term capacity will be improved by the realignment.',\n",
       " 'The lot had been vacant.',\n",
       " 'His faith may be lacking.',\n",
       " \"I don't mind camping for a couple nights in the fall and spring.\",\n",
       " 'Me too?',\n",
       " 'They use lethal injection.',\n",
       " 'They are already in Harvard, New York and Georgetown universities.',\n",
       " 'Auditors can issue a limited official use report that has such information.',\n",
       " 'Dishonest polls is not the point being made.  ',\n",
       " 'However, what is that?',\n",
       " 'Quit holding John responsible.',\n",
       " 'I think more people vote during the presidential elections than local elections.',\n",
       " 'The partition problem of Kashmir remained unresolved.  ',\n",
       " 'The AMIGA model finds the capital investment.',\n",
       " 'Only the outbound attributable cost and inbound revenue change in lines 1 and 2 in columns 5 through 8.',\n",
       " 'Muller was with soldiers.',\n",
       " 'Lawyers were at a premium and it was ideal training for future politicians and politics.',\n",
       " 'A person applauded.',\n",
       " 'My dad has a new eighty nine or ninety Chrysler with a transverse mounted V and front wheel drive.',\n",
       " 'The restaurant serves Middle Eastern-Yemenite food.',\n",
       " 'The table shows a summary of the resources in demand.',\n",
       " 'Hotels accept credit cards as payment unless otherwise stated.',\n",
       " 'The mosque in cairo is protected by walls. ',\n",
       " \"It wasn't the Apaches, certainly.\",\n",
       " 'We probably have the same issues as Mexico and Canada with regards to the use of the English language.',\n",
       " 'I was making some things for gifts.',\n",
       " 'There are a few health food stores that can buy the cookies in bulk for a better deal.',\n",
       " 'The Social Security Trust Fund is made up of money. ',\n",
       " 'He said Dave Hanson.',\n",
       " 'The methodology and results were used to determine the federal pay gap.',\n",
       " 'If you are looking for a different options you can take your charter ship to one of the less popular islands nearby.',\n",
       " 'The tomb was decorated with statues.',\n",
       " 'Chambers built a wall between himself and McCarthy and Nixon.',\n",
       " 'Tupperence felt better after this event.',\n",
       " 'It was no more than a living tomb.',\n",
       " 'The castle was secure due to its proximity to a cliff.',\n",
       " 'I hoped we could be better than that.',\n",
       " 'His head hurt terribly.',\n",
       " 'People would have to cut spending in other areas if food and medical care increased.',\n",
       " 'Jamaican cuisine includes jerk chicken.',\n",
       " 'She was startled by the man and turned when he said I beg your pardon. ',\n",
       " 'They had a temporary program in place.',\n",
       " 'I played softball with the church team for the fun of it.',\n",
       " 'The review by OMB included a description of the reason for the rules.',\n",
       " \"The police don't show up if nobody is injured.\",\n",
       " 'There was another witness.',\n",
       " 'Plants that emit mercury are not necessary.',\n",
       " 'Since our kids are all grown and gone, we have the pets to fill their spots.',\n",
       " 'I work for Hal Ammon.',\n",
       " 'Yeah the average right now.',\n",
       " 'Tourists can view the company of handweavers work, but the historic company have less distinctive products now due to becoming a popular attraction.',\n",
       " 'Dublin is a dense city made for foot travel.',\n",
       " 'You can buy the item at the giftshop on your visit.',\n",
       " \"There is a disagreement as to whether Hubbell's tax fraud charges are being conducted in a proper manner.\",\n",
       " 'Prevention of marine incidents for U.S. economic reasons.',\n",
       " 'There are a number of animals on the wall panels.',\n",
       " 'Please be quiet for a second; I cannot hear and will talk to you in a second.',\n",
       " 'He recruited everyone for this year.',\n",
       " 'He is, actually. ',\n",
       " 'There are many artefacts in Town End which date back to the 1600s, including laundry tools and cookware. ',\n",
       " 'There were villages in the south.',\n",
       " 'Federal funds are dependent upon the populous which they serve.',\n",
       " \"There's a lot of stuff we need to do in this country.\",\n",
       " 'Wheeler said that the program was established because of the changeover concern. ',\n",
       " \"It's hard to tell how things have been kept up and their age because they vary so much from place to place. \",\n",
       " \"It's interesting, I heard it while watching the TV.\",\n",
       " 'You are allowed to appeal the death sentence. ',\n",
       " \"They found Mrs. Inglethorp's will, which left all her fortune to Alfred Inglethorp.\",\n",
       " 'There was a governor fight in Texan politics',\n",
       " 'TV is the most important medium for showing history.',\n",
       " 'Yes, I do that as well.',\n",
       " 'Kirstie Alley plays an older ex-model in the tv show.',\n",
       " \"Don't they just run all over the place?\",\n",
       " 'I am able to.',\n",
       " 'There are people that let you rent their kilns to finish pieces at their shop.',\n",
       " \"We noticed that the teachers say things we don't agree with.\",\n",
       " 'The amount of money you spend does not always indicate your level of success.',\n",
       " 'The Kowloon Canton Railway makes over five stops between the station and Sheung Shui.',\n",
       " 'NJP provides internet-based  help and assistance for those seeking legal help.',\n",
       " 'I had car trouble in the cold weather. ',\n",
       " \"Malaysia is filled with so many natural and man-made wonders that's set to go beyond its tourist's expectations.\",\n",
       " \"A blade went through Kal's chest.\",\n",
       " 'Are you capable of passing judgment on markings of fingers?',\n",
       " 'There is a question about the gender of the announcer.',\n",
       " 'The Mir was launched into orbit by the Soviet Union.',\n",
       " 'We transferred the engine and transmission into another body.',\n",
       " 'I am about five minutes from Rhode Island, not in Cape Cod. ',\n",
       " 'You value your origins a lot.',\n",
       " 'If you want to see what happens, strike the match.',\n",
       " 'He responded.',\n",
       " 'The pro bono need is urgent. ',\n",
       " 'Certain contracts are going to need to be signed to proceed.',\n",
       " \"The Loire's sandbanks become islands in the summer.\",\n",
       " 'Females might perform higher on the screening tests if the cut points are reduced.',\n",
       " 'GAO might only give telephone or email messages.',\n",
       " 'There are instances of this in some large bodies.',\n",
       " 'Are you looking for a new car?',\n",
       " \"If you've got it, spend it.\",\n",
       " 'They were pushing the pace.',\n",
       " \"Greenhouse gases and climate change are being worked on by the President's Cabinet.\",\n",
       " 'There were fattening foods in the refrigerator.',\n",
       " 'It used to be paper 7.',\n",
       " 'The role of Pine Tree has changed over the past twenty years.',\n",
       " 'We can cut out all the background noise when we hook it in.',\n",
       " 'LSC recipients may provide legal aid to foreign people.',\n",
       " 'What is the purpose of acquisition to support?',\n",
       " 'They might have been put there in a rush.',\n",
       " 'We held a friendly morning meal, one of the best since the event occurred.',\n",
       " 'If it is needed to contact the patient the information will not be leaked.',\n",
       " 'Seven thousand men blockaded the city.',\n",
       " 'There are different ways to fish.',\n",
       " 'White raised his eyes to mine slowly.',\n",
       " 'The diving experience is considered better in Sinai.',\n",
       " 'He is usually depicted with the blue face as Krishna, an after-effect of swallowing a poison that threatened the world, and is often represented sporting with the milkmaids he seduced, or playing a flute.',\n",
       " 'There are news networks.',\n",
       " 'Hire a Quad Runner to explore the desert in a more novel, self-drive way.',\n",
       " 'Companies know that they have to deliver the best quality products quickly.  ',\n",
       " 'That darn little animal dance.',\n",
       " 'I work solo and also own a 1980 Trans Am.',\n",
       " 'There are open-air concerts in Ballsbridge.',\n",
       " 'Roman has a lot of experience.',\n",
       " 'Other New Territories villages are hard to access.',\n",
       " 'He had a hook for one arm.',\n",
       " 'People say college is expensive.',\n",
       " 'Visiting the quarries is a great way to learn how ancient Egyptians worked with stone.',\n",
       " 'They are shallow people.',\n",
       " 'we have access to a recreation center which is pretty much just across the street from our office',\n",
       " 'The kiosk is an attempt at easing the flood of litigants unable, or refusing, to hire attorneys. ',\n",
       " 'To see and check the lab.  ',\n",
       " 'The German waited too long and gave Tommy an advantage.',\n",
       " \"I think parents have a greater responsibility in what concerns their children's education than school.\",\n",
       " 'This law demands agencies define various goals for acquisition programs.',\n",
       " 'This is obnoxious and snobby.',\n",
       " 'Advanced humbug means Cold Mountain.',\n",
       " 'These trappings are what come from the billion-dollar price tags associated with them.',\n",
       " 'Physicians are doctors for children.',\n",
       " 'Between the light, there were many riders, quickly riding through the groups of foot soldiers.',\n",
       " 'I want it to be easier to do.',\n",
       " 'There are specific transmission methodologies for classified reports.',\n",
       " 'Professor Cleveland worked as a Skadden Fellow for two years.',\n",
       " 'His wife, Linda, was a teacher.',\n",
       " 'When something is a sure thing, hackneyed ideas and foolish money is sure to follow.',\n",
       " 'Tuppence replied right away.',\n",
       " 'Legislation that would cause a decline in using coal as a source of energy will not be supported by the Administration.',\n",
       " 'Some publications can be rather cavalier without fear.',\n",
       " \"The six-pointed stars that you'll see all over the country look similar to the Jewish Star of David.\",\n",
       " 'Treasury securities interest held by revolving funds.',\n",
       " '924,000 cases were closed in 1999.',\n",
       " 'Styles St. Mary was further from the station than Styles Court. ',\n",
       " 'The ammonia business has been facing competition from the urea business.',\n",
       " \"63 skeletons were found beyond the stage, in the Gladiator's Barracks.\",\n",
       " \"My husband is helping me with he garden, but he doesn't really love doing it.\",\n",
       " 'I would see shows in London often.',\n",
       " 'I am alright in hot weather but it is hard for me to tolerate cold weather.',\n",
       " 'There is a sign near the railway station that depicts the portuguese involvement in Japan.',\n",
       " 'Resolution 98-011 focuses on the presence requirement for representations of aliens.',\n",
       " 'The people who labor outside are optimistic and kind.',\n",
       " \"I've always had a hidden desire to solve crimes.\",\n",
       " 'If you have fresh dirt I can see that.',\n",
       " \"It makes me feel good to see people be concerned about our nation's expenses.\",\n",
       " \"The paintings of Renoir, Monet and Pissarro were at Nadar's house.\",\n",
       " 'Some men do not seem to care, but eventually I think so.',\n",
       " 'I have been thinking about volunteering for the Asian center.',\n",
       " 'She has wasted time and paper.',\n",
       " 'The only thing I wanted to do, was tell you something.',\n",
       " 'The site is now occupied by a chapel, the foundation might be a century old at the very least.',\n",
       " 'Unions will make things worse, they are bad. ',\n",
       " 'The Old Cataract Hotel is at its southern end.  ',\n",
       " 'The APHIS held more than three public hears during the comment period.',\n",
       " 'In the Edo period, gardens were favored and built on spacious grounds.',\n",
       " \"Monet wrote about the tragedy that is reaching the end of one's own life.\",\n",
       " 'I generally like to prepare heavy sauces.',\n",
       " 'Genuine Leather Goods.',\n",
       " 'Agencies have a set of performance goals that senior executives should help achieve.',\n",
       " '\"Old man Rysdale backs the bill,\" Tuppence stated.',\n",
       " 'To show them you mean business, you need a lawyer.',\n",
       " 'This is done for the interests of the community.',\n",
       " 'It gave her pause.',\n",
       " 'Federal agencies have long had difficulty delivering capabilities on time.',\n",
       " 'It is rumored that Rupert Murdoch is interested in buying it and turning it into a West Coast version of the New York Post.',\n",
       " 'It runs ads for its supporters at shows and strikes business deals, but insists it is not commercial.',\n",
       " 'Rounding may cause inaccurate totals.',\n",
       " 'And not a single person will act in a more kind manner.',\n",
       " 'Pro se assistance empowers people to get help and make the court system work better. ',\n",
       " 'Cruises go around Lake Annecy which begin from the Thiou river.',\n",
       " 'Many special rate arrangements would be stopped.',\n",
       " 'He had no reason to claim the kinship.',\n",
       " \"If an agency loses a big account, it's not uncommon for them to fire everyone who was associated with that account. \",\n",
       " 'After completing the performance evaluation period, the employee gets paid the full amount.',\n",
       " 'Some editorial cartoons show people unattractively.',\n",
       " 'Cost-of-services areas are areas where most of power sales are likely to occur.',\n",
       " \"Ca'daan looked towards the campsite.\",\n",
       " 'He created an intricate plan for the next night.',\n",
       " 'I see, wow.',\n",
       " 'Some of the modern buildings that were erected in their place are not admired today.',\n",
       " 'The lieutenant with the supply wagons was there.',\n",
       " 'I thought I heard them so I left quickly.',\n",
       " 'Edmund Morris created a world called Dutch World, a place where made up characters can play with all children.',\n",
       " 'Among the four programs, merger discussions continued and 2 of the programs did not vote in march 2000.',\n",
       " 'Desirable outcomes are achieved in part by identifying design risks early.',\n",
       " 'There are no more one-word battles.',\n",
       " 'Whittington growled and said \"Yes, and perhaps it doesn\\'t,\"',\n",
       " 'There was a guy who went to Leavenworth.',\n",
       " \"The point of David Frum's argument in the Gay Marriage dialogue seems to boil down to Look at how severely and for the worse marriage has changed in the last 30 years.\",\n",
       " 'Cruise ships are host to well-lit parties.',\n",
       " 'Those are the towns beside Lake Como. ',\n",
       " 'Unless you are eating a potato that has been cooked in the oven.',\n",
       " 'These vouchers are not the type that conservatives tend to prefer. ',\n",
       " 'Kubrick made some masterpieces.',\n",
       " 'Congress sometimes wants the progress of agencies examined by us.',\n",
       " 'The Hadassah Medical Centre was built on Mount Scopus in 1925.',\n",
       " 'It contains a small museum.',\n",
       " 'if a person wants to eat you understand that',\n",
       " \"It wasn't done by TI people.\",\n",
       " \"We didn't have too much ice, but there was some. \",\n",
       " 'Saladin was a great Muslim warrior in 1187.',\n",
       " \"French people increasingly recognize that some immigrants recognize national culture and add spice to the country's cuisine.\",\n",
       " 'The office gets state and federal funding.',\n",
       " 'I am an old lady that likes you a lot.',\n",
       " 'The new head of the Chicago Museum of Contemporary Art was just hired away from Disney.',\n",
       " 'There are only two people not in Texas.',\n",
       " 'Advertisers urge us to do things.',\n",
       " 'They were made on site by observers, constituted an inquiry, and were longitudinal.',\n",
       " 'A fleet of donkeys ferries passengers from the town to the port.',\n",
       " 'The idea of youth is a fault that is overcome easily by growth.',\n",
       " 'The fire destroyed it. ',\n",
       " 'Write offs are used to remove financial information. ',\n",
       " 'Its population has increased by over two million.',\n",
       " 'I talked to someone about the education system and credit card usage.',\n",
       " '70 minutes prior to the Pearl Harbor attack, the Pacific War began.',\n",
       " 'The king died in 1598 and was buried in a family tomb.',\n",
       " 'Installing control technologies in order to reduce emissions required resources.',\n",
       " 'In Costa Rica, immigration by American citizens is forbidden.',\n",
       " 'The rate difference should be equivalent to the average incremental savings.',\n",
       " 'I really look forward to class reunions.',\n",
       " 'The entire season played out before it was revealed that it was a dream Pam was having.',\n",
       " 'The cape contains wild goats and an isolated lighthouse and a watchtower.',\n",
       " 'A few of them were pieces from museums.',\n",
       " 'Families in Lexington had horses.',\n",
       " 'The votes of this month are unimportant.',\n",
       " 'Saint-Malo is still known as a fishing port of some importance.',\n",
       " \"San'doro said Gods.\",\n",
       " 'There is a demand of information.',\n",
       " 'The first Carey Award was held by VA in 1992.',\n",
       " 'Kennedy and the ABA worked together.',\n",
       " 'After pouring hot water all over your body, a gloved attendant will exfoliate your skin until you are pink and glowing.',\n",
       " 'The Rivas boy was killed a year ago.',\n",
       " 'You said that he went northward.',\n",
       " 'Local Moroccan rebels defeated it in 1923.',\n",
       " 'Reporters follow a framework outlining basic principles. ',\n",
       " 'The agencies work to partner with customers and other stakeholders.',\n",
       " \"Some agencies' websites contain separate links for each of their rules and procedures. \",\n",
       " 'He was under order to not have any contact with the three travelers unless he was afraid of an attack. ',\n",
       " '6 million was handed to the Legal Aid Society for court-appointed lawyers.',\n",
       " 'William Kellie Smith was a rubber planter who died while he was visiting Scotland in the mid-1920s.',\n",
       " 'Some of the best big-game fishing in Europe is found off the coast of Algarve.',\n",
       " 'All bets are off when on the phone with Mom.',\n",
       " 'Concentrations and depositions will be reported as a percent reduction.',\n",
       " 'Celeste was embarrassed. ',\n",
       " 'Their children are living okay there.',\n",
       " 'We have to prepare people to go into the mines.',\n",
       " 'The Tate Gallery had an exhibit.',\n",
       " \"Danny Dan Marino's first year was the time when I got into football\",\n",
       " 'His critics think his failures define him.',\n",
       " 'The pattern is thrown out if the evidence is overly inconsistent.',\n",
       " 'The differences between ultimate fighting and boxing are few.',\n",
       " 'We do it by lethal injection and it is more civilized.  ',\n",
       " 'I was pretty tied up this weekend because I was planting flowers.',\n",
       " 'Let down by Linda Tripp.',\n",
       " 'Many information security experts are more and more interested in measuring positive and negative effects of security on business operations.',\n",
       " 'He sounds like you.',\n",
       " 'And there is a significant discrepancy.',\n",
       " 'I have two boys.',\n",
       " 'Owners plan to keep the facility under budget.',\n",
       " 'We do not stop plugging.',\n",
       " 'Richardson needed to make Picasso into a serious artist and an honorable man.',\n",
       " 'We had the tents secure by late that night.',\n",
       " 'Poirot continued in a minute or two...',\n",
       " 'The current secondary highway system was the subject for him.',\n",
       " 'The ex-con is now a sportscaster.',\n",
       " 'Egypt became the first Arab state to recognize Israel in 1979 when other Arab states were aghast.',\n",
       " 'I was afraid of heights but my friend talked me into it.',\n",
       " 'I only saw a single chance.',\n",
       " 'The Spanish people find eating well to be among their favorite things.',\n",
       " 'If you try to pay with a credit card, he might increase the price to cover card charges.',\n",
       " 'You need to go to Sleeping With The Enemy.',\n",
       " 'The major hardware for an FGD system includes the flue gas duct system.',\n",
       " 'South Audley Mansions was close to Park Lane.',\n",
       " 'You can either wear striped white shirts or pastel-coloured shirts.',\n",
       " \"that's very good.\",\n",
       " 'We are responsible for improvements in managing federal resources. ',\n",
       " 'The person named Benedykt Ossolinsky lived to be at least 39 years old. ',\n",
       " 'Visitors can walk from the Porta Nuova to a famous wooden crucifix.',\n",
       " 'Handicraft stores are used to display products from different locations.',\n",
       " \"Will's articles are only good in regards to sports\",\n",
       " \"I think Major Dad is on at eight o'clock.\",\n",
       " \"It isn't going to happen like you waltzing in and telling them things.\",\n",
       " 'There are plans to turn the house into a museum.',\n",
       " 'Jon stuck a rapier in the groin of the man. ',\n",
       " 'These days the hotels and villas comprise a beautiful resort.',\n",
       " 'Italy and Germany took sides with Franco and the Soviet Union took sides with the Republicans.',\n",
       " 'Getting up to par will take them another two to five years.',\n",
       " 'There is plenty of art and history to be absorbed in the residence.',\n",
       " 'A constituent service is what public officers call it.',\n",
       " 'Jews think of drawings on the MoMA as a diagram of the races teaching circumcision. ',\n",
       " 'Our company buys parts, like other companies.',\n",
       " 'Ginsburg is exceptionally tough to refuse according to the lawyers.',\n",
       " 'News reports the death of JFK Jr. seemed almost predestined.',\n",
       " 'There was a possibility that she had come to help him.',\n",
       " 'At the top of staircases, they often used arched front doors.',\n",
       " 'The crime is seen as something horrible.',\n",
       " 'Mail and phone conversations were used to disseminate information.',\n",
       " 'He sighed heavily at the end.',\n",
       " \"St. Mark's basin has historically been seen as a gateway to the Adriatic.\",\n",
       " \"Value of reallocated allowances can be calculated and subtracted from each unit's cost/\",\n",
       " \"Success doesn't result from simply being more efficient.\",\n",
       " \"The Times-Mirror flagship Los Angeles Times learned that navel-gazing doesn't always impress.\",\n",
       " 'Congress did not support apartheid in South Africa.',\n",
       " \"The Arch of Septimius Severus' friezes have served as a source of inspiration for many Baroque and Renaissance sculptors. \",\n",
       " \"They didn't require it to be unanimous. \",\n",
       " 'Hokkaido has a lot of fun winter sports.',\n",
       " 'There are additional reporting standards for financial audits.',\n",
       " 'Glendalough is 20,000 hectares and has spectacular scenery.',\n",
       " 'Even if you are bad with language, attempt to pick up some Italian expressions. ',\n",
       " 'IRS is going to expand the job responsibilities of permanent employees.',\n",
       " \"Jon caught Adrin's rapier tip in the guard of his own rapier.\",\n",
       " 'The engine could be heard through the open window.  ',\n",
       " 'Whew of last week!',\n",
       " 'Understanding such controls can be vital when planning an audit.',\n",
       " \"I kept praying that we were fast enough to get out of the bomb's impact zone.\",\n",
       " 'The Eye was a threat to the North.',\n",
       " 'Nationalism had always somewhat been there and the Scottish people tried to gain their self-worth back.',\n",
       " \"I'm in a different department than you. \",\n",
       " 'There are at least two art galleries on Belford Road.  ',\n",
       " 'The carriage made a lot of noise.',\n",
       " 'My oldest child will be six years old soon. ',\n",
       " 'Everyone played well together except for the quarterbacks.',\n",
       " \"Maybe we don't want those people to vote.\",\n",
       " 'Centrelink pays a number of Australian government benefits.',\n",
       " 'They have $100 to spend in December.',\n",
       " 'I live to the north of Pittsburgh.',\n",
       " 'You could be in New York having adventures right now.',\n",
       " 'It still have room for improvement.',\n",
       " 'Michael Stapleton designed the beautiful stucco ceiling in the Dublin Writers Museum.',\n",
       " 'Aversion therapy applied to excessive drinking.',\n",
       " 'Jon was looked to by the others.',\n",
       " 'The sword had jewels in its twisted hand-guard.',\n",
       " 'Ponder on the United States Postal Service.',\n",
       " 'Information for data and strategies.',\n",
       " 'AO mail is almost always heavier than LC mail.',\n",
       " \"The NAACP's project will get almost $50,000.\",\n",
       " 'Because Parcells makes his players think they can win, they do.',\n",
       " 'For the past 1000 years, it was the largest market in the Near East.  ',\n",
       " \"They don't say you'll always have great career conditions.\",\n",
       " 'Software and hardware  solutions depend on the needs of each business.',\n",
       " 'All major faith traditions have a core value that our work exemplifies.',\n",
       " 'Post-Cold War peace resulted in the creation of The Liberal Humanitarians.',\n",
       " 'Examples of control activities ',\n",
       " 'Dave followed the man while they spoke to each other.',\n",
       " \"If they don't have good fries, we're getting Chinese. \",\n",
       " 'They do whatever they can.',\n",
       " 'The last items purchased are the first out in a LIFO model.',\n",
       " 'They won-- something, due to their perseverance, unlike the students of Columbia.',\n",
       " 'He entertained Frank Zaappa after taking office.',\n",
       " \"Don't stand, Dave Hanson.\",\n",
       " 'The Dept. of Defense does not have the criteria policy to measure design stability and process controls.',\n",
       " \"The risk assessment finds potential risks in a system that's being developed.\",\n",
       " 'I can see the same things as you do and I agree with you.  ',\n",
       " 'Drew watched the three riders ride away.',\n",
       " 'The route of the tours may change from month to month, depending on what is being restored. ',\n",
       " 'King Alfonso V of Aragon reunited it with Sicily.',\n",
       " 'Well, I will look into that.',\n",
       " 'We might get more tonight.',\n",
       " 'This is, I said, my fiancee.',\n",
       " 'He looked down at himself and felt revulsion.',\n",
       " \"They weren't able to push it through Maryland.\",\n",
       " \"The small chapel's Madonna di Nicopeia dates to the 10th century.\",\n",
       " \"Watching the news isn't always relaxing.\",\n",
       " 'The reagent is more often than not limestone, and is handled and processed onsite.',\n",
       " 'According to critics, some immigrants should not be naturalized.  ',\n",
       " 'The town was peaceful.',\n",
       " 'The comments stated have to be verified.',\n",
       " 'There are a number of walks around the woodlands.',\n",
       " 'Many CFOs are in charge of the Results Act.',\n",
       " 'The return on us assets abroad exceeds us foreign owned assets ',\n",
       " 'The aquarium is the biggest in all of Europe and was added on the quincentennial anniversary.',\n",
       " 'While we were travelling, I realized many things. ',\n",
       " 'The trouble started after.',\n",
       " 'Quite inventive and popular with the younger crowd.',\n",
       " 'The appendix has more information.',\n",
       " 'Alexander Graham Bell, inventor of the telephone, was born in that building at the corner of Charlotte Square.',\n",
       " 'Enjoy it! ',\n",
       " 'Are we sure of this?',\n",
       " 'You will pass through the historic towns of Toulouse, Albi, and Montpellier on your way to the final destination.',\n",
       " 'He was offered a lot of money by trial lawyers.',\n",
       " 'They all have their campaign ribbons from the war of HR22',\n",
       " 'The patients were more likely to be young men with low incomes. ',\n",
       " 'Over the time period in the original sentence, she has gone to school, worked in her field, moved to her own place, and will start her next degree soon.',\n",
       " 'I set aside my worries.',\n",
       " 'The Bok House is transformed into a restaurant.',\n",
       " \"How's everything where you are as far as that goes? \",\n",
       " 'At the time, I was pregnant.',\n",
       " 'SOuth Padre is a little farther.',\n",
       " 'You can see the five windmills that sit above the town from here.  ',\n",
       " 'Not the first, but the second.',\n",
       " \"How about a failing TV show about a political campaign that's collapsing?\",\n",
       " \"It's likely we'll be speaking to you again.\",\n",
       " 'Psychiatric care was provided for the family members.',\n",
       " \"He'd have to protect the shield from the sun things.\",\n",
       " 'This is a problem between us that nearly split us up, but she is working on it.',\n",
       " 'Children really like the marionette shows in the gardens.',\n",
       " 'There is good food at the Henry Grattan lounge.',\n",
       " '21 heads on display were found in 1977.',\n",
       " 'The agency estimated that about two thirds of the firms would be affected.',\n",
       " 'The visitors are all fascinated by the place.',\n",
       " 'Las vegas was built to serve travelers.',\n",
       " 'I was sitting on the edge of my seat.',\n",
       " 'You want to make him come out and how himself.',\n",
       " \"Washington is being looked at to fix the economy's negative aspects.\",\n",
       " 'I heard that he was not a team player.',\n",
       " \"Well, then, what's the point of reading any kind of fiction?\",\n",
       " 'It must be taken into account the objectivity of case studies.',\n",
       " 'Until the building of the Eiffel Tower in 1898, it was the tallest structure in the world at 450 ft high.',\n",
       " 'Warranties offer extensions. ',\n",
       " 'I receive items in the mail.',\n",
       " \"When there's more than one family in a house. \",\n",
       " \"The discrepancy isn't important.\",\n",
       " 'It was nice speaking to you, good luck with your hopes of buying a Winnebago.',\n",
       " \"The president's mother visited.\",\n",
       " 'Close on the banks of the River exists the Nile Hilton Hotel, known by the locals as well as tourists.',\n",
       " 'The pair ends up in the middle of the drama.',\n",
       " 'What is it you do? ',\n",
       " \"Don't skip out on the chance for a boat cruise departing from Porto.\",\n",
       " \"That's the house that Tuppence was in.\",\n",
       " 'This column is free of history.',\n",
       " 'Roda Island is home to the palace.',\n",
       " 'Did you ever go?',\n",
       " 'Many companies avoid unneeded difficulties.',\n",
       " \"American's children are very affected by poverty \",\n",
       " 'Señor Juanito commented that.',\n",
       " 'The book was solidly finished by a discussion of political writing.',\n",
       " \"I'm sure that the Massachusetts miracle is over.\",\n",
       " 'There are methods for approaching things like graphic data displays, and for building complex tabulations to check for relationships.',\n",
       " '27% are black and not Hispanic.',\n",
       " 'Others assign 7,000 cops to the scene but deny that any specific threat exists.',\n",
       " 'The gateway to Provence is Orange.',\n",
       " 'The view of the lit up mont at night is wonderful.',\n",
       " 'The congress reduced the military personnel budget without compromising readiness. ',\n",
       " 'She turned to him. ',\n",
       " 'Vegas is constantly transforming.',\n",
       " \"There wasn't a lot done to integrate more than one data source.\",\n",
       " 'The Soviets were quick to crush the uprising in Hungary.',\n",
       " 'Good Chinese and Indian restaurants, hotels, and a variety of English-style tearooms are all over the Tanah Rata.',\n",
       " 'By the way, what ARE we going to do?',\n",
       " 'I have asthma and cannot job.',\n",
       " 'I was overwhelmed, it was too much to work with. ',\n",
       " \"I couldn't recall.\",\n",
       " \"Ramses couldn't accept defeat.\",\n",
       " 'Dowd was upset that Republicans treated Bush and Clinton differently.',\n",
       " 'Jon thought Susan should move in to the caves.',\n",
       " 'Disease was an additional burden on the people.',\n",
       " 'Every facility has waste disposal requirements based on regulations. ',\n",
       " 'The subprime lending industry is being marketed aggressively.',\n",
       " 'Saving the Social Security surpluses produces unified budget surpluses for almost 20 years.',\n",
       " 'The wrong doing was the search and not the conviction.',\n",
       " 'Flytrap is a subject in published articles.',\n",
       " 'You are responsible for your actions regardless of age.',\n",
       " 'I met a famous detective in Belgium who angered me.',\n",
       " 'Emperor Divers has been around for a long time.',\n",
       " 'Suppose whoever did it thought Johnny was dead, took his horse, and left him.',\n",
       " 'Out of all the liberals, he is my favorite.',\n",
       " 'The American was unsure exactly how much money was left.',\n",
       " 'Stephen King has a new book, but he is no longer working with his former publisher.',\n",
       " 'A number of clients receiving adequate legal advice.',\n",
       " \"There is a mausoleum of Akbar's in Skiandra that is just 6 miles from Agra.\",\n",
       " 'the goods needed by businesses and governments relies on the amount available resource for investment. ',\n",
       " 'George Shultz is the other gentile who enters my story.',\n",
       " 'When a plausible explanation has been developed, data analysis ends.  ',\n",
       " 'Elemental load time varies with the number of pieces being loaded.',\n",
       " 'Lucifer!',\n",
       " 'The 13th century temple has been plundered and ripped by weather.  ',\n",
       " \"Stephanopoulos posed a question about the relevancy of candidates' private lives. \",\n",
       " 'This was hard by any measure.  ',\n",
       " 'His weakness has always been young foolishness.',\n",
       " 'Rothko left a lasting impression on many through his art. ',\n",
       " 'All could be sent to a specific town area or project paid for by someone.',\n",
       " 'Poor people in more than a couple counties in Atlanta receive help from the Atlanta Legal Aid. ',\n",
       " 'The nice interior features were all hidden behind junk.',\n",
       " 'The CFO Council Fellow Program started in 1998.',\n",
       " 'His clients will handle better because of their histories and legal standings.',\n",
       " 'The wardmaid was to blame. ',\n",
       " 'The city remained lively until 1801.',\n",
       " 'Obtaining better data on risk factors and measuring the value of security controls are a priority of managers.',\n",
       " 'The view was hard to see due to the light.',\n",
       " 'All of them get tested every three months',\n",
       " 'The Moorish army built a fort.',\n",
       " 'Someone actually blew it.',\n",
       " 'I consider Wilkins to be a great fool. ',\n",
       " 'Close up of the character played by George Clooney, the handsome and expressive actor, before the sound of gunfire rings through the air.',\n",
       " 'the swords are in trouble if they did not begin their route now.',\n",
       " 'Two days seems pretty realistic for a national election.',\n",
       " 'Cold River has a microclimate that makes it a great stopover.',\n",
       " 'Imagine sitting by the lake.',\n",
       " 'There is a fantastic view of the blue and whitewashed city from this perspective.',\n",
       " \"A description in the Immigration and Nationality Act forms the basis of the program's name.\",\n",
       " 'They receive tickets within their school for recycling newspaper or cans.',\n",
       " 'Osman Gazi defeated the Byzantine in 1301.',\n",
       " 'Sir James rubbed his chin. ',\n",
       " \"I didn't spend as much time as I should have.\",\n",
       " 'The traditional and Gregorian music is performed in old lemosan.',\n",
       " 'He has some books that are best sellers. ',\n",
       " 'These numbers do not show the adjustment for changes in real income over time.',\n",
       " 'Pat Robertson said that he was a marine officer during the Korean War.',\n",
       " 'The Government Management Reform Act was passed in 1994.',\n",
       " 'It would only require some notable politicians to halt putting lobbyists in high offices.',\n",
       " \"I don't understand religion.\",\n",
       " 'Ways to obtain decriptions',\n",
       " 'LSC was one of the organizations selecting participants.',\n",
       " 'Duly noting the factors inherent in the ledger item.',\n",
       " 'The analysis is completed.',\n",
       " 'Changes in visibility can be estimated with the REMSAD model.',\n",
       " 'No one knows how the third plant ended up in Jamaica.',\n",
       " 'Bring Kennewick Man to his tribe if the evidence is valid.',\n",
       " 'Gingrich and Barr both have thrown Molotov cocktails from the back benches.',\n",
       " 'The match can end occasionally within seconds.',\n",
       " 'The candidate most likely to win will be the one that addresses education reform, how to improve the economy and military security. ',\n",
       " 'A peak occurs at a discount of 7a. ',\n",
       " \"It's nice to wander around the historic neighborhood.\",\n",
       " 'Rice is an important staple.',\n",
       " 'The simple granite sarcophagus is in a small chamber.',\n",
       " 'Members had to trust the other members in order to disclose their true thoughts.',\n",
       " 'There is a mountain chalet at the edge of Serra de Agua.',\n",
       " 'It is important to realize that interventions are not superfluous.',\n",
       " 'Clinical research in emergency medicine does not have much funding.',\n",
       " 'There are still no burials allowed, even after the laws have been changed.',\n",
       " 'The act sets up a series of pilot audits. ',\n",
       " \"5 USC governs the departure of an employee who doesn't accrue and receives pay whether they are present or absent.\",\n",
       " \"Shouldn't have entertained that idea.\",\n",
       " 'There was an awful row.',\n",
       " 'The majority of senior executives are in its regional offices. ',\n",
       " \"He's starting to get braver with his activities.\",\n",
       " 'Congress has the power to alter practices involving aliens.',\n",
       " 'Port Antonio is located in Jamaica, a tiny island east of Central America.',\n",
       " 'Tyson claimed that the things he did were things he would normally never do.',\n",
       " 'Because of the cost of insurance, we set money aside for the car every month.',\n",
       " \"It doesn't matter who the anchor is, the program is still World News Tonight with Peter Jennings.  \",\n",
       " 'Herman kept himself from striking a conversation during the convention.',\n",
       " 'It is pleasant to see people you know when you are in a foreign culture.',\n",
       " \"If you missed any links in the article, you can click further to read about Kerr's thoughts.\",\n",
       " 'If you like hiking, you can go at Latrigg, which is an easier ascent.',\n",
       " 'The Old One made a shadow.',\n",
       " 'The beaches with fine sand are in the north.',\n",
       " 'Intent is about understanding the process.',\n",
       " 'Neighborhoods on the outskirts.',\n",
       " 'Take care of the cats! ',\n",
       " 'The people at the top of the organization recognize the value of the training.',\n",
       " 'Once it was around -60 degrees.',\n",
       " 'Lowering extinction can be used to estimate numerical measures related to human perceptions. ',\n",
       " 'Keep social security.',\n",
       " 'He had a hapless manner, I noted thoughtfully.',\n",
       " \"The government messed up in the Isaac's case.\",\n",
       " 'The reason for hacks cannot always be ascertained.',\n",
       " 'Offline readers put advertisers on websites in the same position as print ones. ',\n",
       " 'The goal of most Washington hearings is to talk and score points.',\n",
       " 'The architectural style of houses in the Islamic culture is based on buildings built along fountains.',\n",
       " 'Mary Ann G. McMarrow is a Chief Justice.',\n",
       " 'There is where you died.',\n",
       " \"It's not important when you're only at a living wage.\",\n",
       " 'It is unsure whether domes and towers were in the original plans.',\n",
       " 'We made a long drive to San Diego, then to Texas, and then through Colorado before going home.',\n",
       " 'He seems like a good guy, but he would have sold out his own mother.',\n",
       " 'The aquarium can be interesting on rainy afternoons.',\n",
       " 'Employees of several commercial firms are allowed to keep their frequent flyer awards.',\n",
       " 'Pity the poor attorney.',\n",
       " 'He was the youngest of four siblings.',\n",
       " \"Mike Smith's retirement hindered the Phillies' offensive capability.\",\n",
       " 'Good purchases to make include liquor and tobacco products.',\n",
       " 'He thought the hearth fire had died.',\n",
       " 'The witty comments made by the speaker impressed everyone.',\n",
       " 'Making negative comments about the critics of Kosovo.',\n",
       " 'I know some people who did it which was indirectly related.',\n",
       " 'The all-inclusive hotel is for couples only.',\n",
       " 'Her skin lost color, and her eyes widened.',\n",
       " 'The third man received a wound in the abdomen. ',\n",
       " 'Bork ate it silently but made a face at the taste.',\n",
       " 'Thanks was given. ',\n",
       " 'Forfeited property and inventory are not applicable.',\n",
       " 'The bureaucrats and Desdemona lived in two different buildings.',\n",
       " 'Indeed, one of a kind. ',\n",
       " 'On the higher levels of the town hall, Umbrian and Tuscan paintings are on show.',\n",
       " 'Senior executives are expected to create strategies for achieving company objectives and goals.',\n",
       " \"Today's finest craftsmen copy the work of the historic artistic geniuses.\",\n",
       " 'The clerk had on a summer dress.',\n",
       " 'I thought of the old lady in the house.',\n",
       " 'Most often, the public cannot see the sheet.',\n",
       " 'Ephesus, Priene, and Miletus were part of a league of city-states in the region of Ionia.',\n",
       " 'Sex will become more a practice of please rather than procreation, as parents will choose to clone themselves or genetically engineer offspring. ',\n",
       " 'The contrast was obvious to me after a period of time.',\n",
       " 'He ought to have went outside and roughed some person up or peddled drugs.',\n",
       " \"I don't understand what you tell me about the girl, said Mr. Carter.\",\n",
       " 'Shinto festivals are where the real drama is at.',\n",
       " 'The farmer and the fisherman may come to Paris for different reasons from everyone else.',\n",
       " 'I did not see him much for quite a few years.  ',\n",
       " 'Just like Slate anticipated.',\n",
       " \"Driving north of Alicante will lead you to the rocky, moon-like landscape of the Cabeao d'Or mountains.\",\n",
       " 'More challenging choices require a guide.',\n",
       " \"There's no science definite enough that committee decisions don't have to examine it.\",\n",
       " 'Yes, I was able to finish the house.',\n",
       " 'There are more than 200 difference species of animals housed there.',\n",
       " 'The Commission has general rules for mail classification.',\n",
       " 'Just north of Fort-de-France is the extension of the capital city, Schoelcher.',\n",
       " 'I am not sure what yet, but something to generate interest.',\n",
       " 'A basic security program that demonstrates importances.',\n",
       " 'I guess they just want to have an average conversation.',\n",
       " 'Ser shrugged and stayed behind the trio.',\n",
       " 'Hilliker is going to be the leader.',\n",
       " \"I'll charge it since I know that money will be there.\",\n",
       " \"Newsweek hints at 13 members of Heaven's Gate might be traversing the Southwest.\",\n",
       " 'I can point you to a political glossary that might be helpful.',\n",
       " \"You can't rely on social security. The government needs a new plan.\",\n",
       " 'I want to get all my calls before the plug is pulled on the program.',\n",
       " 'Cocktails or something else.',\n",
       " 'Five centuries',\n",
       " 'The rooms all have air-conditioning and cable.',\n",
       " 'Beach clubs, luxury hotels, and a casino are situated alongside three miles of fine sand.',\n",
       " 'The area to the north is a boring desert, but to the south lie sandstone peaks overlooking the nearby coasts.',\n",
       " 'The lag adjustment distributes the mortality incidence over five years and discounts mortality benefits at a rate of three percent.',\n",
       " 'Steve and Cokie Roberts confirmed that they were there too. ',\n",
       " 'The alternative I use is interesting.',\n",
       " \"Students coming from high schools and junior colleges don't have a background for Technological Study.\",\n",
       " 'They were south of Tucson.',\n",
       " 'Dave Hanson was upset with something.',\n",
       " 'There is an iconic painting portraying death.',\n",
       " 'Access time can be defined as the amount of time a carrier needs to stray from the route to make the delivery. ',\n",
       " \" It began with the sight of his uncle's face.\",\n",
       " 'My cats could probably do that.',\n",
       " 'An agreeing opinion to R2000-1',\n",
       " 'Netscape provided Mr. Bork with his argument.',\n",
       " 'She said there was nor formal training pertaining to this back then.',\n",
       " 'The second recommendation has David Fiellin suggesting it be removed.',\n",
       " 'Although certainly true.',\n",
       " 'The Academy of Motion Picture Arts and Sciences enforced the blacklist.',\n",
       " \"Sekhmet is a goddess with a lion's head.\",\n",
       " \"I don't like their self-centeredness.\",\n",
       " 'Diego de Velazquez sailed from Hispaniola in 1511.',\n",
       " 'Cold pressed oils are available.',\n",
       " 'The speaker wants the conditions to repeat themselves. ',\n",
       " 'Fragments of a monument to Jupiter were discovered near the Notre-Dame.',\n",
       " 'US corporations tried to fill the financial gap in for the network.',\n",
       " 'The cakes here are equally great.',\n",
       " 'More artsy people would be better than lawyers and lobbyists.',\n",
       " 'The hosue is beautifully maintained.',\n",
       " 'All this...what is it for?',\n",
       " 'Its architecture is from the 18th century.',\n",
       " 'Penguins can be seen at the Edinburgh Zoo in summer.',\n",
       " \"I think they're still around.\",\n",
       " \"Launched in 1995, The Department of Labor's original Retirement Savings Education Campaign\",\n",
       " 'It seems glamorous to take drugs.',\n",
       " 'If you want to watch the shepherds at work, you can go to the auditorium.',\n",
       " \"There are many ways that Susan's talent can help.\",\n",
       " 'Everyone loves camping, especially with the scouts.',\n",
       " \"Blumenthal's current fifteen minutes of fame is building recognition that could help him if he decides to run for governor.\",\n",
       " 'ED patients have alcohol problems that need to be intervened.',\n",
       " 'What is their wager? ',\n",
       " 'risk assessments were performed.',\n",
       " 'I got out of the bath while I was babbling.',\n",
       " 'Saint Johnswort does cure depression for some people.',\n",
       " 'The communities in need of equal justice services are becoming more and more diverse.',\n",
       " \"I really admire Peter DeNiro's acting.\",\n",
       " 'Someone please tell a synonym for future.',\n",
       " 'Being a person who rents not even caring about that.',\n",
       " \"Ajami observes men love trouble that they're familiar with.\",\n",
       " 'I think that this case might be a crisis.',\n",
       " 'The bell beside this temple has no rope.',\n",
       " 'A minimum of a 1.33 Cpk was the level at which four of the five companies we visited wanted their critical processes at, and many had goals of achieving higher Cpks.',\n",
       " 'I am going to come up with another group.',\n",
       " 'Most likely one large thing about them.',\n",
       " 'Old Madeira is reborn on the terrace with a mini village, little palheiro, weaving loom containing house, and old fashioned shop.',\n",
       " '\"If you want the others to leave, they will leave, and we will stay.\"',\n",
       " 'The staff has 13 attorneys.',\n",
       " 'But an extra smoke stack is usually unnecessary.',\n",
       " 'From before the wars.',\n",
       " 'This has made Starr squirmy. ',\n",
       " 'That is precisely my point, Albert. ',\n",
       " 'Straight men are needed for forming the question. ',\n",
       " 'A dark-skinned man emerged from behind the cantina.',\n",
       " \"I've done it already.\",\n",
       " 'She loves fetch, and she will get it from anywhere I throw it.',\n",
       " 'Query that is more detailed.',\n",
       " 'African migrants who rest in the Balearics sometimes stay for summer.',\n",
       " 'Tommy was tailing this man when she last saw him.',\n",
       " \"Grisham believes that we don't respect the homeless.\",\n",
       " 'it makes a big difference',\n",
       " 'Sincerely',\n",
       " \"The PDFA's campaign has sunk in at major newsrooms.\",\n",
       " 'Jon asked Susan whether Thorn and the Kal could take another twenty.',\n",
       " 'Political tension between head and heart has gone by the wayside.',\n",
       " 'You can buy less expensive leather goods on the Via Tritone.',\n",
       " 'The inside lacked a lot of space.',\n",
       " 'I noticed that Monsieur Lawrence could have gone to the poison cupboard.',\n",
       " 'Japanese boys use the carp as fit model.',\n",
       " 'The evidence is actually pretty depressing. ',\n",
       " 'That was quick since he always had on hand all of the ingredients that he has used with success in the past.',\n",
       " 'The afghans I viewed in the shop were for cross-stitching.',\n",
       " 'For over a century the most exciting method up the Victoria Peak is by funicular.',\n",
       " 'The family is going to fight over his money.',\n",
       " 'You push a button and you hear a train, and the toot toot sound',\n",
       " 'The Explorer said that larger worlds existed.',\n",
       " 'This document contains information on Real GDP.',\n",
       " 'White ethics are no longer the dominant force.',\n",
       " 'Retired people are benefiting from my money.',\n",
       " 'The Christian Reconquest had taken over 13th century Ibiza.',\n",
       " 'Some of the tiles have dents and the finish is peeling.',\n",
       " 'You have limits.',\n",
       " 'for spring break, for example',\n",
       " \"The theory isn't right if the facts don't fit it.\",\n",
       " 'I had some interest in it.',\n",
       " 'The biographer was forced to remove many quotes from their previously published work. ',\n",
       " 'When people grow up they sometimes join the Peace Corps.',\n",
       " 'This is a very terrible business, Monsieur Poirot, he said.',\n",
       " 'The center is still a fishing village.',\n",
       " 'She was definitely allowed to want an explanation. ',\n",
       " 'You can still find cheap hand-painted plates, jugs, and jars at Madeira.',\n",
       " 'In areas outside of the valley, Nepal did not see growth.',\n",
       " 'Terrible idea!',\n",
       " 'Our approach would reduce state resources that are needed to conduct modeling.',\n",
       " 'I could imagine the word but not say it correctly.',\n",
       " 'Deferred maintenance can be estimated by engineering calculations.',\n",
       " \"There's no scientific evidence for a link between a child's exposure to homosexuality and their later sexual orientation.\",\n",
       " 'The model predicts percentages that are quite accurate.',\n",
       " 'You were given away because you tried to steal.',\n",
       " 'There was a deadline imposed to get Franklin working in two weeks, otherwise there would be an issue.',\n",
       " 'It appears that she enjoys when it happens.',\n",
       " 'Everyone who works in law must work together.',\n",
       " 'Gulbenkian got one of the greatest collections in all of Europe.',\n",
       " \"You don't have the strength to tell the salamander what to do, but you reacted well in the crisis.\",\n",
       " 'He bent his legs.',\n",
       " 'If things go well, you support the President.',\n",
       " 'What branch were you in?',\n",
       " 'Publishing houses can give less attention to editing books.',\n",
       " 'The IRS has a standard relating to how taxpayers are treated.',\n",
       " 'There are four fences, and you can only go past the second one if you are a member of the imperial family, or a high-ranking priest.',\n",
       " 'Scotland was starting to gain its power to make policies for their own country back ',\n",
       " 'Pol Pot was a Cambondian dictator.',\n",
       " 'He was a small, naked, old man. ',\n",
       " 'There is a week in which you are the only person he can turn to when the government is shut down.',\n",
       " 'I bought a ninety because I really liked the eighty-eight that I had.  ',\n",
       " 'You can go trekking to see hill stations.',\n",
       " 'Martha is the CEO of a company named Omnimedia.',\n",
       " 'American Webheads would be stunned by European telecom monopolies.',\n",
       " 'Tripp was considered by some to be treacherous and beneath contempt.',\n",
       " 'He drank a bit too much sometimes.',\n",
       " 'Bush caters to the religious right with his faith.',\n",
       " 'The museum celebrates painting, sculpture, architecture, and more.',\n",
       " 'He put drops in a test tube and sealed it.',\n",
       " 'It was a partial plug and pray.',\n",
       " 'The hondo is the most popular attraction for visitors to Kiyomizu.',\n",
       " \"It's fun seeing players play for opposite teams.\",\n",
       " \"Open air sightseeing isn't recommended around lunch time.\",\n",
       " 'The Savoy bar has is well-known for its active nightlife.',\n",
       " \"Sather Karf's words ended abruptly before the others' shouts could overpower them.\",\n",
       " 'A black hood and cloak was worn by the other man.',\n",
       " 'The garment industry is trying to make formal attire more popular.',\n",
       " 'I used to think Dallas was better than Houston but now Dallas looks just like Houston.',\n",
       " 'The housing market is in a slump right now.',\n",
       " 'You mean with pro ball.',\n",
       " 'A new evidence has shown up at lunch time.',\n",
       " 'There would be some follow-up assessments.',\n",
       " 'The LSC Performance Criteria is related to the ABA Standards for Prividers of Civil Legal Services to the Poor.',\n",
       " 'There are multiple Greek Islands in the Aegean.',\n",
       " 'Are the can crushers hard workers?',\n",
       " \"The initiative's values, purposes and objectives were agreed upon broadly.\",\n",
       " 'The sun had landed close to the horizon.',\n",
       " 'A previous conversation between Poirot and Evelyn Howard came to my mind.',\n",
       " 'It has two cutting blades that trim off extra seam fabric.',\n",
       " \"Lido dei Maronti is located near Sant'Angelo.\",\n",
       " 'LSC assists many who receive grants in the state. ',\n",
       " 'If the RWC is under 25%, the concentrations vary.',\n",
       " 'If you see a play in ancient theaters they make it worthwhile.',\n",
       " 'I breathed in deeply.',\n",
       " \"Sports Night is an annoying sitcom that's a cross between David Mamet and thirtysomething.\",\n",
       " \"To add to that, contrary to recent assertions, we are not seeking the minutes of these meetings or related notes of the Vice President's staff.\",\n",
       " \"Kosovar's aspirations to become independent were at their highest that year.\",\n",
       " 'Dorcas saw her holding a paper.',\n",
       " 'The Torah codes debate will probably be ended by the publication of the rebuttal paper.',\n",
       " 'The landing impact must have broken his bones.',\n",
       " 'In addition to this some organizations used e-mail to communicate non-sensitive information to their memberships.',\n",
       " \"That is excellent exercise and I don't participate enough of that kind of exercise either. \",\n",
       " \"I don't really care for the laying out process.\",\n",
       " 'Reports have stated that Arafat is no longer a terrorist and is being a peaceful citizen in america',\n",
       " 'Honda Accords and Toyota Camrys are the vehicles most criminals choose to steal.',\n",
       " \"Central bankers in Japan completely assume that price stability is an overriding goal, but it's probably not. \",\n",
       " 'The chance of graduating high school drops when your family moves during your school years.',\n",
       " 'The Mercato San Lorenzo is very interesting.',\n",
       " 'The subclasses of mail exist today',\n",
       " 'The Jets played against Minnesota last week.',\n",
       " 'The other servants were paying rapt attention.',\n",
       " 'The distribution of particulate matter air quality improvements is estimated.',\n",
       " 'The Manchurian Candidate is horny,',\n",
       " \"He blamed plath's suicide on fate and astrology. \",\n",
       " 'The risks associated with particular cost reduction plans are included in the process of information management.',\n",
       " 'We reviewed a large body of literature on management reform.',\n",
       " 'Helms was charmed when Annan called him.',\n",
       " 'The brows and jaw were new to the man. ',\n",
       " \"I hope I'm never in the position to be put in a nursing home.\",\n",
       " 'Shoppers respect mail-order firms more than department stores in terms of their ability to keep things in stock.',\n",
       " 'They kicked in a door.',\n",
       " 'An active government cultural policy was developed not too long ago.',\n",
       " 'We cannot eat out for a while because of the kid.',\n",
       " 'A topic of debate was whether or not the candidates took votes away from the Republicans.',\n",
       " 'There are other islands in the chain besides Hawaii.',\n",
       " 'Are you positive?',\n",
       " 'I agree, he did it.',\n",
       " 'We are unsure of that.',\n",
       " 'Malaysia is located two degrees above the equator.',\n",
       " \"He may be a competent replacement but I can't be sure.\",\n",
       " 'Will the analysis of literature abruptly end?',\n",
       " 'Tommy asked for Tuppence in a weak manner. ',\n",
       " 'The definition of the term affected EGU establishes which electricity generating units are covered by the WRAP program.',\n",
       " 'The idea of packing light to travel is charming.',\n",
       " 'Something was put in her room every evening and she warmed it up in the night, whenever she felt like it. ',\n",
       " 'Mister Kells has a ledger in the tack room.  ',\n",
       " 'I am not playing. ',\n",
       " 'Scooters and mopeds can be selected.',\n",
       " 'The Lombards had a loose confederation of fiercly independent duchies to control the interior.',\n",
       " 'Pick the Washington Post',\n",
       " 'Do you have the key that goes into this door?',\n",
       " 'Yes, a lot of the time.',\n",
       " 'We should let them start recording.',\n",
       " \"The rain poured over the corners of Adrin's hat.\",\n",
       " 'Thanks to cheaper moviemaking via digital technology, filmmaking will be less tasking.',\n",
       " 'That is my desire, said the girl yearningly.',\n",
       " \"Half of the mother's genes are transferred into unfertilized eggs. \",\n",
       " \"That's a bargain, yes.\",\n",
       " 'The Prepared for Office of Policy, Planning and Evaluation is part of the EPA.',\n",
       " \"They don't really eat that type of food in Israel.\",\n",
       " 'Someone noted our altitude was decreasing faster than it should.',\n",
       " 'It cannot be said that innocence is an absolute certainty.',\n",
       " 'The project adopted principles.',\n",
       " \"Reporters don't need sexy artists the likes of Davis for interesting news. \",\n",
       " 'Clothing stores can be found throughout the city.',\n",
       " 'Sella village is located beneath a large plateau.',\n",
       " 'Kemp politely asked for a diet coke.',\n",
       " 'He began suggestion the united sate would have large growth rates, just like South Korea predicted.',\n",
       " 'It takes a lot of work and effort to establish useful, efficient, operating institutions.',\n",
       " 'He had a problem with the hiring freeze on the agencies.',\n",
       " 'I just get to do oil changes and filters and the like since computers do everything else.',\n",
       " \"Hopefully, they'll resolve that problem soon\",\n",
       " 'Hick thought Eleanor would divorce FDR.',\n",
       " 'I can see some potential changes, that might not cost much, but still help.',\n",
       " 'It was destroyed by fire in 1781.',\n",
       " \"It doesn't cost anything to get in shape.\",\n",
       " 'At home I wear things like sweatpants.',\n",
       " \"Administrative tasks are one of the jobs that'd be available.\",\n",
       " 'Julius is being encouraged.',\n",
       " 'This small collection contains 16 El Grecos.',\n",
       " \"That's good to hear. \",\n",
       " 'You have almost arrived at the point I think.',\n",
       " 'Pokemon blends TV with games very well.',\n",
       " 'Jaye and Erik sold the building.',\n",
       " 'HUD has discovered the controlled business disclosures.',\n",
       " 'A Pulse Jet Fabric Filter could be implemented when ACI is installed.',\n",
       " 'Office Behemoth is a dominant buyer.',\n",
       " 'When you change your oil you have to dispose of it responsibly.',\n",
       " 'The northerner fell down.',\n",
       " 'It is sometimes wise to have a higher price in mind when haggling.',\n",
       " 'In the 1999/2000 year, IRD went through 7.2 million tax returns.',\n",
       " 'The network was capable of carrying your traffic.',\n",
       " 'The gladiator said that he wanted to see you again. ',\n",
       " 'My brother has gotten frost before, and his crops died.',\n",
       " 'The LSC reconfigured the Bay Area.',\n",
       " 'Champagne-Ardennes',\n",
       " 'It means that the race to find mates leads a lot of people save a lot of money.',\n",
       " \"Because you're having fun\",\n",
       " 'No one of royal blood had ever slept on the bed.',\n",
       " \"He made sure that she knew he didn't know what love was until she came along. \",\n",
       " \"After all this time, yes, that's still doing it\",\n",
       " 'The transfer paper shows the transfer.',\n",
       " 'L.A. has a reputation because of previous earthquakes, flood, crime and scandals.',\n",
       " 'Their theory is about struggling to escape.',\n",
       " 'The Explorer said that it was a native.',\n",
       " 'I saw Home Alone because it was recommended.',\n",
       " 'He does not believe in some public spaces.',\n",
       " 'The southern Greek mainland began to fall under the sway of city-states.',\n",
       " 'The Integrated Planning Model helped to make electricity generation emmission projections',\n",
       " \"Ca'daan's uncle knew a lot about the situation.\",\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNLI dataset from GLUE\n",
    "mnli = load_dataset(\"glue\", \"mnli\", split = \"train\").select(range(50_000))\n",
    "mnli = mnli.remove_columns(\"idx\")\n",
    "mnli = mnli.filter(lambda x: True if x[\"label\"] == 0 else False)\n",
    "# mnli[\"hypothesis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16875it [00:00, 59228.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare data and add a soft negative\n",
    "train_dataset = {\n",
    "    \"anchor\": [],\n",
    "    \"positive\": [],\n",
    "    \"negative\": []\n",
    "}\n",
    "soft_negatives = mnli[\"hypothesis\"]\n",
    "\n",
    "random.shuffle(soft_negatives)\n",
    "\n",
    "for row, soft_negative in tqdm(zip(mnli, soft_negatives)):\n",
    "    train_dataset[\"anchor\"].append(row[\"premise\"])\n",
    "    train_dataset[\"positive\"].append(row[\"hypothesis\"])\n",
    "    train_dataset[\"negative\"].append(soft_negative)\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the evaluator\n",
    "\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "# Create an embedding similarity evaluator for stsb\n",
    "val_sts = load_dataset(\"glue\", \"stsb\", split = \"validation\")\n",
    "evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1 = val_sts[\"sentence1\"],\n",
    "    sentences2 = val_sts[\"sentence2\"],\n",
    "    scores = [score/5 for score in val_sts[\"label\"]],\n",
    "    main_similarity = \"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train the model as before but with MNR loss instead\n",
    "\n",
    "from sentence_transformers import losses, SentenceTransformer\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "embedding_model = SentenceTransformer('bert-base-uncased')\n",
    "\n",
    "# Loss Function\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model = embedding_model)\n",
    "\n",
    "# Define the training Arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir = \"mnrloss_embedding_model\",\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    warmup_steps = 100,\n",
    "    fp16 = False,\n",
    "    eval_steps = 100,\n",
    "    logging_steps = 100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6067ca8fca14332b570c87d9c3f39d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3ea534d5b04dc185c0ff886e9f1227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3065, 'grad_norm': 5.35787296295166, 'learning_rate': 5e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1064, 'grad_norm': 4.354597568511963, 'learning_rate': 3.831775700934579e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0789, 'grad_norm': 3.512991189956665, 'learning_rate': 2.663551401869159e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0639, 'grad_norm': 1.0885411500930786, 'learning_rate': 1.4953271028037382e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0645, 'grad_norm': 5.52611780166626, 'learning_rate': 3.2710280373831774e-06, 'epoch': 0.95}\n",
      "{'train_runtime': 321.8728, 'train_samples_per_second': 52.428, 'train_steps_per_second': 1.64, 'train_loss': 0.12088996804121768, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=528, training_loss=0.12088996804121768, metrics={'train_runtime': 321.8728, 'train_samples_per_second': 52.428, 'train_steps_per_second': 1.64, 'total_flos': 0.0, 'train_loss': 0.12088996804121768, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model = embedding_model,\n",
    "    args = args,\n",
    "    train_dataset = train_dataset,\n",
    "    loss = train_loss,\n",
    "    evaluator = evaluator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pearson_cosine': 0.8023413256058616, 'spearman_cosine': 0.8056238363922879}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate our trained model\n",
    "evaluator(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning an Embedding Model\n",
    "\n",
    "### Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNLI dataset from GLUE\n",
    "# 0 = entailment, 1 = neutral, 2 = contradiction\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"glue\", \"mnli\", split = \"train\"\n",
    ").select(range(50_000))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns(\"idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding similarity evaluator for stsb\n",
    "val_sts = load_dataset(\"glue\", \"stsb\", split = \"validation\")\n",
    "evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1 = val_sts[\"sentence1\"],\n",
    "    sentences2 = val_sts[\"sentence2\"],\n",
    "    scores = [score/5 for score in val_sts[\"label\"]],\n",
    "    main_similarity = \"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "from sentence_transformers import losses, SentenceTransformer\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Loss Function\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model = embedding_model)\n",
    "\n",
    "# Define the training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir = \"finetuned_embedding_model\",\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    warmup_steps = 100,\n",
    "    fp16 = False,\n",
    "    eval_steps = 100,\n",
    "    logging_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319888dc945e49c5a18f98a47dec02dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34518c1bd4004ab1b5e2309697cdc2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Column 'hypothesis' is at index 1, whereas a column with this name is usually expected at index 0. Note that the column order can be important for some losses, e.g. MultipleNegativesRankingLoss will always consider the first column as the anchor and the second as the positive, regardless of the dataset column names. Consider renaming the columns to match the expected order, e.g.:\n",
      "dataset = dataset.select_columns(['hypothesis', 'entailment', 'contradiction'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1618, 'grad_norm': 3.7395999431610107, 'learning_rate': 5e-05, 'epoch': 0.06}\n",
      "{'loss': 0.113, 'grad_norm': 3.6550345420837402, 'learning_rate': 4.6582365003417636e-05, 'epoch': 0.13}\n",
      "{'loss': 0.1237, 'grad_norm': 2.660911798477173, 'learning_rate': 4.316473000683528e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1195, 'grad_norm': 2.34393572807312, 'learning_rate': 3.9747095010252904e-05, 'epoch': 0.26}\n",
      "{'loss': 0.1069, 'grad_norm': 5.350363731384277, 'learning_rate': 3.632946001367054e-05, 'epoch': 0.32}\n",
      "{'loss': 0.1027, 'grad_norm': 1.264941930770874, 'learning_rate': 3.291182501708818e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1119, 'grad_norm': 2.589830160140991, 'learning_rate': 2.9494190020505813e-05, 'epoch': 0.45}\n",
      "{'loss': 0.1039, 'grad_norm': 3.024691581726074, 'learning_rate': 2.6076555023923443e-05, 'epoch': 0.51}\n",
      "{'loss': 0.1043, 'grad_norm': 2.005444288253784, 'learning_rate': 2.2658920027341084e-05, 'epoch': 0.58}\n",
      "{'loss': 0.1046, 'grad_norm': 4.572895050048828, 'learning_rate': 1.9241285030758715e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0932, 'grad_norm': 2.7334372997283936, 'learning_rate': 1.5823650034176352e-05, 'epoch': 0.7}\n",
      "{'loss': 0.1109, 'grad_norm': 2.5162789821624756, 'learning_rate': 1.2406015037593984e-05, 'epoch': 0.77}\n",
      "{'loss': 0.1062, 'grad_norm': 4.064367294311523, 'learning_rate': 8.988380041011621e-06, 'epoch': 0.83}\n",
      "{'loss': 0.1069, 'grad_norm': 2.4062306880950928, 'learning_rate': 5.570745044429255e-06, 'epoch': 0.9}\n",
      "{'loss': 0.1079, 'grad_norm': 3.5764219760894775, 'learning_rate': 2.15311004784689e-06, 'epoch': 0.96}\n",
      "{'train_runtime': 187.8416, 'train_samples_per_second': 266.182, 'train_steps_per_second': 8.321, 'train_loss': 0.11080329965797664, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1563, training_loss=0.11080329965797664, metrics={'train_runtime': 187.8416, 'train_samples_per_second': 266.182, 'train_steps_per_second': 8.321, 'total_flos': 0.0, 'train_loss': 0.11080329965797664, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model = embedding_model,\n",
    "    args = args,\n",
    "    train_dataset = train_dataset,\n",
    "    loss = train_loss,\n",
    "    evaluator = evaluator\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pearson_cosine': 0.8463357002991696, 'spearman_cosine': 0.8468585071608757}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate our trained model\n",
    "evaluator(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from sentence_transformers import InputExample\n",
    "from sentence_transformers.datasets import NoDuplicatesDataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a small set of 10_000 documents for the cross-encoder\n",
    "dataset = load_dataset(\"glue\", \"mnli\", split = \"train\").select(range(10_000))\n",
    "mapping = {2:0, 1:0, 0:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 73585.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data Loader\n",
    "gold_examples = [\n",
    "    InputExample(texts = [row[\"premise\"], row[\"hypothesis\"]], label = mapping[row[\"label\"]])\n",
    "    for row in tqdm(dataset)\n",
    "]\n",
    "\n",
    "gold_dataloader = NoDuplicatesDataLoader(gold_examples, batch_size = 32)\n",
    "\n",
    "# Pandas.DataFrame for easier data handling\n",
    "gold = pd.DataFrame(\n",
    "    {\n",
    "        \"sentence1\": dataset[\"premise\"],\n",
    "        \"sentence2\": dataset[\"hypothesis\"],\n",
    "        \"label\" : [mapping[label] for label in dataset[\"label\"]]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**# STEP-1 : Using this gold_dataset we train our cross-encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c27e2a624242a4b93297fda15c03ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd84cd587f44eeeaf017d17b7306ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train a cross_encoder on the gold dataset\n",
    "cross_encoder = CrossEncoder(\"bert-base-uncased\", num_labels = 2)\n",
    "cross_encoder.fit(\n",
    "    train_dataloader = gold_dataloader,\n",
    "    epochs = 1,\n",
    "    show_progress_bar = True,\n",
    "    warmup_steps = 100,\n",
    "    use_amp = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP-2**\n",
    "After training our cross-encoder, we use the remaining 400,000 sentence pairs (from our original dataset of 50,000 sentence pairs) as our silver dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'mnli' at /Users/qbit-glitch/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c (last modified on Sat Mar 15 18:27:34 2025).\n"
     ]
    }
   ],
   "source": [
    "# Prepare the silver dataset by predicting labels with the cross-encoder\n",
    "silver = load_dataset(\n",
    "    \"glue\", \"mnli\", split = \"train\"\n",
    ").select(range(10_000, 50_000))\n",
    "pairs = list(zip(silver[\"premise\"], silver[\"hypothesis\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-3**: We will use our fine-tuned cross-encoder to label the sentence pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841befe93ac04ee5857424aa0ddeab1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Label the sentence pairs using our fine-tuned corss-encoder\n",
    "output = cross_encoder.predict(\n",
    "    pairs, apply_softmax = True,\n",
    "    show_progress_bar = True\n",
    ")\n",
    "\n",
    "silver = pd.DataFrame(\n",
    "    {\n",
    "        \"sentence1\" : silver[\"premise\"],\n",
    "        \"sentence2\": silver[\"hypothesis\"],\n",
    "        \"label\" : np.argmax(output, axis = 1)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a silver and gold dataset, we simply combine them and train our embedding model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine gold + silver\n",
    "data = pd.concat([gold, silver], ignore_index = True, axis = 0)\n",
    "data = data.drop_duplicates(subset = [\"sentence1\", \"sentence2\"], keep = \"first\")\n",
    "train_dataset = Dataset.from_pandas(data, preserve_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our evaluator\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding similarity evaluator for stsb\n",
    "val_sts = load_dataset(\"glue\", \"stsb\", split = \"validation\")\n",
    "evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1 = val_sts[\"sentence1\"],\n",
    "    sentences2 = val_sts[\"sentence2\"],\n",
    "    scores = [score/5 for score in val_sts[\"label\"]],\n",
    "    main_similarity = \"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model as before except now we use the augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import losses, SentenceTransformer\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "embedding_model = SentenceTransformer('bert-base-uncased')\n",
    "\n",
    "# Loss Function\n",
    "train_loss = losses.CosineSimilarityLoss(model = embedding_model)\n",
    "\n",
    "# Define the training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir = \"augmented_embedding_model\",\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    warmup_steps = 100,\n",
    "    fp16 = False,\n",
    "    eval_steps = 100,\n",
    "    logging_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90e5f695c814b1995a96ee86cd8fb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0608309e91495da73bfc94aba47f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2177, 'grad_norm': 1.6327018737792969, 'learning_rate': 5e-05, 'epoch': 0.06}\n",
      "{'loss': 0.159, 'grad_norm': 1.3607633113861084, 'learning_rate': 4.6582365003417636e-05, 'epoch': 0.13}\n",
      "{'loss': 0.1417, 'grad_norm': 1.288422703742981, 'learning_rate': 4.316473000683528e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1435, 'grad_norm': 1.1119270324707031, 'learning_rate': 3.9747095010252904e-05, 'epoch': 0.26}\n",
      "{'loss': 0.1409, 'grad_norm': 1.0912528038024902, 'learning_rate': 3.632946001367054e-05, 'epoch': 0.32}\n",
      "{'loss': 0.1376, 'grad_norm': 1.5323593616485596, 'learning_rate': 3.291182501708818e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1351, 'grad_norm': 1.212894320487976, 'learning_rate': 2.9494190020505813e-05, 'epoch': 0.45}\n",
      "{'loss': 0.1323, 'grad_norm': 1.1308387517929077, 'learning_rate': 2.6076555023923443e-05, 'epoch': 0.51}\n",
      "{'loss': 0.1366, 'grad_norm': 1.2236522436141968, 'learning_rate': 2.2658920027341084e-05, 'epoch': 0.58}\n",
      "{'loss': 0.1337, 'grad_norm': 1.0599873065948486, 'learning_rate': 1.9241285030758715e-05, 'epoch': 0.64}\n",
      "{'loss': 0.1328, 'grad_norm': 1.2602442502975464, 'learning_rate': 1.5823650034176352e-05, 'epoch': 0.7}\n",
      "{'loss': 0.1294, 'grad_norm': 0.9497034549713135, 'learning_rate': 1.2406015037593984e-05, 'epoch': 0.77}\n",
      "{'loss': 0.1277, 'grad_norm': 1.0856826305389404, 'learning_rate': 8.988380041011621e-06, 'epoch': 0.83}\n",
      "{'loss': 0.1303, 'grad_norm': 1.3833718299865723, 'learning_rate': 5.570745044429255e-06, 'epoch': 0.9}\n",
      "{'loss': 0.1295, 'grad_norm': 0.9105015993118286, 'learning_rate': 2.15311004784689e-06, 'epoch': 0.96}\n",
      "{'train_runtime': 738.9236, 'train_samples_per_second': 67.663, 'train_steps_per_second': 2.115, 'train_loss': 0.14130667761511628, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1563, training_loss=0.14130667761511628, metrics={'train_runtime': 738.9236, 'train_samples_per_second': 67.663, 'train_steps_per_second': 2.115, 'total_flos': 0.0, 'train_loss': 0.14130667761511628, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model = embedding_model,\n",
    "    args = args,\n",
    "    train_dataset = train_dataset,\n",
    "    loss = train_loss,\n",
    "    evaluator = evaluator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pearson_cosine': 0.6940822174538952, 'spearman_cosine': 0.7114926647534681}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning: Transformer Based Sequential Denoising Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/qbit-\n",
      "[nltk_data]     glitch/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/qbit-\n",
      "[nltk_data]     glitch/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download additional Tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flat sentences from our data and remove any labels that we have to mimic an unsupervised setting\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, load_dataset\n",
    "from sentence_transformers.datasets import DenoisingAutoEncoderDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48353/48353 [00:03<00:00, 15319.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a flat list of Sentences\n",
    "mnli = load_dataset(\"glue\", \"mnli\", split = \"train\").select(range(25_000))\n",
    "flat_sentences = mnli[\"premise\"] + mnli[\"hypothesis\"]\n",
    "\n",
    "# Add noise to out input data\n",
    "damaged_data = DenoisingAutoEncoderDataset(list(set(flat_sentences)))\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = {\"damaged_sentence\" : [], \"original_sentence\":[]}\n",
    "\n",
    "for data in tqdm(damaged_data):\n",
    "    train_dataset[\"damaged_sentence\"].append(data.texts[0])\n",
    "    train_dataset[\"original_sentence\"].append(data.texts[1])\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'damaged_sentence': 'I at to salaries',\n",
       " 'original_sentence': \"I'm looking at ways to raise salaries.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "# Create an embedding similarity evaluator for stsb\n",
    "val_sts = load_dataset(\"glue\", \"stsb\", split = \"validation\")\n",
    "evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1 = val_sts[\"sentence1\"],\n",
    "    sentences2 = val_sts[\"sentence2\"],\n",
    "    scores = [score/5 for score in val_sts[\"label\"]],\n",
    "    main_similarity = \"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the training as before but with the [CLS] token as the pooling strategy instead of the mean pooling of the token embeddings. In the TSDAE paper, this was shown to be more effective since mean pooling loses the position information, which is not the case when using the [CLS] token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import models, SentenceTransformer\n",
    "\n",
    "# Create your embedding model\n",
    "word_embedding_model = models.Transformer(\"bert-base-uncased\")\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), \"cls\")\n",
    "embedding_model = SentenceTransformer(modules = [word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our sentence pairs, we will need a loss function that attempts to reconstruct the original sentence using the noise sentence, namely DenoisingAutoEncoderLoss. By doing so, it will learn how to accurately represent the data. It is similar to masking but without knowing where the actual masks are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import losses\n",
    "\n",
    "# Use the denoising auto-encoder loss\n",
    "train_loss = losses.DenoisingAutoEncoderLoss(\n",
    "    embedding_model, tie_encoder_decoder = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, training our model works the same as we have seen several times before but we lower the batch size as memory increases with this loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir = \"tsdae_embedding_model\",\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    warmup_steps = 100,\n",
    "    fp16 = False,\n",
    "    eval_steps = 100,\n",
    "    logging_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40596bbc4e664f8f98b051c5fc5cb1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61473797a4564cab8df985c07f2d5632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3023 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.1927, 'grad_norm': 9.87906265258789, 'learning_rate': 5e-05, 'epoch': 0.03}\n",
      "{'loss': 5.0671, 'grad_norm': 7.317001819610596, 'learning_rate': 4.8289428669175505e-05, 'epoch': 0.07}\n",
      "{'loss': 4.6236, 'grad_norm': 8.491228103637695, 'learning_rate': 4.657885733835101e-05, 'epoch': 0.1}\n",
      "{'loss': 4.5012, 'grad_norm': 9.565860748291016, 'learning_rate': 4.486828600752652e-05, 'epoch': 0.13}\n",
      "{'loss': 4.4797, 'grad_norm': 7.688411235809326, 'learning_rate': 4.315771467670202e-05, 'epoch': 0.17}\n",
      "{'loss': 4.4926, 'grad_norm': 8.716361045837402, 'learning_rate': 4.1447143345877524e-05, 'epoch': 0.2}\n",
      "{'loss': 4.4358, 'grad_norm': 16.072032928466797, 'learning_rate': 3.973657201505303e-05, 'epoch': 0.23}\n",
      "{'loss': 4.4552, 'grad_norm': 82.69937133789062, 'learning_rate': 3.802600068422854e-05, 'epoch': 0.26}\n",
      "{'loss': 4.4781, 'grad_norm': 14.16832160949707, 'learning_rate': 3.631542935340404e-05, 'epoch': 0.3}\n",
      "{'loss': 4.5669, 'grad_norm': 10.393641471862793, 'learning_rate': 3.460485802257954e-05, 'epoch': 0.33}\n",
      "{'loss': 4.48, 'grad_norm': 48.447509765625, 'learning_rate': 3.2894286691755046e-05, 'epoch': 0.36}\n",
      "{'loss': 4.4665, 'grad_norm': 11.903848648071289, 'learning_rate': 3.1183715360930556e-05, 'epoch': 0.4}\n",
      "{'loss': 4.4862, 'grad_norm': 12.644669532775879, 'learning_rate': 2.947314403010606e-05, 'epoch': 0.43}\n",
      "{'loss': 4.5272, 'grad_norm': 12.480951309204102, 'learning_rate': 2.7762572699281562e-05, 'epoch': 0.46}\n",
      "{'loss': 4.5574, 'grad_norm': 145.15185546875, 'learning_rate': 2.605200136845707e-05, 'epoch': 0.5}\n",
      "{'loss': 4.3541, 'grad_norm': 12.236113548278809, 'learning_rate': 2.4341430037632572e-05, 'epoch': 0.53}\n",
      "{'loss': 4.3544, 'grad_norm': 15.947933197021484, 'learning_rate': 2.2630858706808075e-05, 'epoch': 0.56}\n",
      "{'loss': 4.3099, 'grad_norm': 12.853243827819824, 'learning_rate': 2.092028737598358e-05, 'epoch': 0.6}\n",
      "{'loss': 4.333, 'grad_norm': 12.292401313781738, 'learning_rate': 1.9209716045159084e-05, 'epoch': 0.63}\n",
      "{'loss': 4.2481, 'grad_norm': 11.7252836227417, 'learning_rate': 1.749914471433459e-05, 'epoch': 0.66}\n",
      "{'loss': 4.2496, 'grad_norm': 9.101014137268066, 'learning_rate': 1.5788573383510094e-05, 'epoch': 0.69}\n",
      "{'loss': 4.246, 'grad_norm': 17.041887283325195, 'learning_rate': 1.4078002052685599e-05, 'epoch': 0.73}\n",
      "{'loss': 4.2012, 'grad_norm': 13.671198844909668, 'learning_rate': 1.2367430721861102e-05, 'epoch': 0.76}\n",
      "{'loss': 4.3364, 'grad_norm': 28.71721076965332, 'learning_rate': 1.0656859391036606e-05, 'epoch': 0.79}\n",
      "{'loss': 4.1732, 'grad_norm': 15.357911109924316, 'learning_rate': 8.946288060212111e-06, 'epoch': 0.83}\n",
      "{'loss': 4.1676, 'grad_norm': 12.547866821289062, 'learning_rate': 7.235716729387617e-06, 'epoch': 0.86}\n",
      "{'loss': 4.1425, 'grad_norm': 18.568382263183594, 'learning_rate': 5.525145398563121e-06, 'epoch': 0.89}\n",
      "{'loss': 4.1402, 'grad_norm': 12.864462852478027, 'learning_rate': 3.814574067738625e-06, 'epoch': 0.93}\n",
      "{'loss': 4.0632, 'grad_norm': 13.567455291748047, 'learning_rate': 2.1040027369141293e-06, 'epoch': 0.96}\n",
      "{'loss': 4.1204, 'grad_norm': 14.984042167663574, 'learning_rate': 3.93431406089634e-07, 'epoch': 0.99}\n",
      "{'train_runtime': 959.9004, 'train_samples_per_second': 50.373, 'train_steps_per_second': 3.149, 'train_loss': 4.472961895204797, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3023, training_loss=4.472961895204797, metrics={'train_runtime': 959.9004, 'train_samples_per_second': 50.373, 'train_steps_per_second': 3.149, 'total_flos': 0.0, 'train_loss': 4.472961895204797, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model = embedding_model,\n",
    "    args = args,\n",
    "    train_dataset = train_dataset,\n",
    "    loss = train_loss,\n",
    "    evaluator = evaluator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pearson_cosine': 0.7436534295971304, 'spearman_cosine': 0.7550700404676484}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate our trained model\n",
    "evaluator(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HandsOnLLMs",
   "language": "python",
   "name": "handsonllms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
