{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Large Language Models\n",
    "\n",
    "## OpenCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an AI generated image of a puppy playing in the snow\n",
    "puppy_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/puppy.png\"\n",
    "image  = Image.open(urlopen(puppy_path)).convert(\"RGB\")\n",
    "caption = \"a puppy playing in the snow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a caption for this image, we can use OpenCLIP to generate embeddings for both.\n",
    "To do so, we load in three models:\n",
    "- A tokenizer for tokenizing the textual input\n",
    "- A preprocessor to process and resize the image\n",
    "- The main model that converts the previous outputs to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/clip-vit-base-patch32\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer to preprocess the text\n",
    "clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "# Load a processor to preprocess the image\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Main model for generating text and image embeddings\n",
    "model = CLIPModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize our input\n",
    "inputs = clip_tokenizer(caption, return_tensors = \"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our input back to tokens\n",
    "clip_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have preprocessed our caption, we can create the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text embedding\n",
    "text_embedding = model.get_text_features(**inputs)\n",
    "text_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can create our image embedding, like the text embedding, we will need to preprocess it as the model expects the input image to have certain characteristics, like its size and shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image\n",
    "processed_image = clip_processor(\n",
    "    text = None,\n",
    "    images = image,\n",
    "    return_tensors = \"pt\"\n",
    ")[\"pixel_values\"]\n",
    "\n",
    "processed_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the results of this preprocessing\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare image for visualization\n",
    "img = processed_image.squeeze(0)\n",
    "img = img.permute(*torch.arange(img.ndim - 1, -1, -1))\n",
    "img = np.einsum(\"ijk->jik\", img)\n",
    "\n",
    "# Visualize the preprocessed image\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the image embedding\n",
    "image_embedding = model.get_image_features(processed_image)\n",
    "image_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these embeddings to calculate how similar they are. To do so, we normalize the embeddings first before calculating the dot product to give us a similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the embeddings\n",
    "text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
    "image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Calculate their similarity\n",
    "text_embedding = text_embedding.detach().cpu().numpy()\n",
    "image_embedding = image_embedding.detach().cpu().numpy()\n",
    "score = text_embedding @ image_embedding.T\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images\n",
    "cat_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/cat.png\"\n",
    "car_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png\"\n",
    "\n",
    "paths = [puppy_path, cat_path, car_path]\n",
    "images = [Image.open(urlopen(path)).convert(\"RGBA\") for path in paths]\n",
    "\n",
    "captions = [\n",
    "    \"a puppy playing in the snow\",\n",
    "    \"a pixelated image of a cute cat\",\n",
    "    \"a supercar on the road with the sunset in the background\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Embed all images\n",
    "image_embeddings = []\n",
    "for image in images:\n",
    "    image_processed = clip_processor(images = image, return_tensors = \"pt\")['pixel_values']\n",
    "    image_embedding = model.get_image_features(image_processed).detach().cpu().numpy()[0]\n",
    "    image_embeddings.append(image_embedding)\n",
    "\n",
    "image_embeddings = np.array(image_embeddings)\n",
    "\n",
    "# Embed all captions\n",
    "text_embeddings = []\n",
    "for caption in captions:\n",
    "    inputs = clip_tokenizer(caption, return_tensors = \"pt\")\n",
    "    text_emb = model.get_text_features(**inputs).detach().cpu().numpy()[0]\n",
    "    text_embeddings.append(text_emb)\n",
    "\n",
    "text_embeddings = np.array(text_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity between images and captions\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim_matrix = cosine_similarity(image_embeddings, text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base figure\n",
    "plt.figure(figsize=(20,14))\n",
    "plt.imshow(sim_matrix, cmap = 'viridis')\n",
    "\n",
    "# Adjust ticks with correct labels\n",
    "plt.yticks(range(len(captions)), captions, fontsize = 18)\n",
    "plt.xticks([])\n",
    "\n",
    "# Visualize\n",
    "for i, image in enumerate(images):\n",
    "    plt.imshow(image, extent = (i-0.5, i+0.5, -1.6, -0.6), origin = \"lower\")\n",
    "\n",
    "# Add the captions at the correct indices\n",
    "for x in range(sim_matrix.shape[1]):\n",
    "    for y in range(sim_matrix.shape[0]):\n",
    "        plt.text(x, y, f\"{sim_matrix[y,x]:.2f}\", ha=\"center\", va=\"center\", size=30)\n",
    "\n",
    "# Remove unnecessary spines\n",
    "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "    plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "# Resize blocks\n",
    "plt.xlim([-0.5, len(captions) - 0.5])\n",
    "plt.ylim([len(captions) + 0.5, -2])\n",
    "plt.title(\"Similarity Matrix\", size = 20)\n",
    "plt.savefig(\"sim_matrix.png\", dpi=300, bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load SBERT-compatible CLIP model\n",
    "model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "# Encode the images\n",
    "image_embeddings = model.encode(images)\n",
    "\n",
    "# Encode the captions\n",
    "text_embeddings = model.encode(captions)\n",
    "\n",
    "# Compute cosine similarities\n",
    "sim_matrix = util.cos_sim(image_embeddings, text_embeddings)\n",
    "print(sim_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Text Generation models Multimodal\n",
    "\n",
    "### BLIP-2 Bridging the modality Gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Multimodal inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processor and the main model\n",
    "blip_processor = AutoProcessor.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    TOKENIZERS_PARALLELISM=True\n",
    ")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    torch_dtype = torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to GPU to speed up inference\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image of a supercar\n",
    "car_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png\"\n",
    "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image\n",
    "inputs = blip_processor(image, return_tensors = \"pt\").to(device, torch.float16)\n",
    "inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Convert to numpy and go from (1, 3, 224, 224) to (224, 224, 3) in shape\n",
    "image_inputs = inputs[\"pixel_values\"][0].detach().cpu().numpy()\n",
    "image_inputs = np.einsum('ijk->kji', image_inputs)\n",
    "image_inputs = np.einsum('ijk->jik', image_inputs)\n",
    "\n",
    "# Scale image inputs to 0-255 to represent RGB values\n",
    "scaler = MinMaxScaler(feature_range=(0, 255))\n",
    "image_inputs = scaler.fit_transform(image_inputs.reshape(-1, image_inputs.shape[-1])).reshape(image_inputs.shape)\n",
    "image_inputs = np.array(image_inputs, dtype=np.uint8)\n",
    "\n",
    "# Convert numpy array to Image\n",
    "Image.fromarray(image_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_processor.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "text = \"Her vocalization was remarkably melodic\"\n",
    "token_ids = blip_processor(image, text=text, return_tensors = \"pt\")\n",
    "token_ids = token_ids.to(device, torch.float16)[\"input_ids\"][0]\n",
    "\n",
    "# Convert input_ids back to tokens\n",
    "tokens = blip_processor.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the space token with an underscore\n",
    "tokens = [token.replace(\"Ġ\", \"_\") for token in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case-1 : Image Captioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an AI-generated image of a supercar\n",
    "image = Image.open(urlopen(car_path)).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert an image into inputs and preprocess it\n",
    "inputs = blip_processor(image, return_tensors = \"pt\").to(device, torch.float16)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate image ids to be passed to the decoder (LLM)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "# Generate text from the image ids\n",
    "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Rorschach image\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/7/70/Rorschach_blot_01.jpg\"\n",
    "image = Image.open(urlopen(url)).convert(\"RGB\")\n",
    "\n",
    "# Generate caption\n",
    "inputs = blip_processor(image, return_tensors = \"pt\").to(device, torch.float16)\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens = 20)\n",
    "\n",
    "generated_text = blip_processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens = True\n",
    ")\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case-2 : Multimodal Chat-Based Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an AI generated image of a supercar\n",
    "image = Image.open(urlopen(car_path)).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual question answering\n",
    "prompt = \"Question: Write down what you see in this picture. Answer: \"\n",
    "\n",
    "# Process both the image and the prompt\n",
    "inputs = blip_processor(image, text = prompt, return_tensors = \"pt\").to(device, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "generated_ids = model.generate(**inputs, max_new_tokens = 30)\n",
    "generated_text = blip_processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens = True\n",
    ")\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_eventhandler(*args):\n",
    "    question = args[0][\"new\"]\n",
    "    if question:\n",
    "        args[0][\"owner\"].value = \"\"\n",
    "\n",
    "        # Create Prompt\n",
    "        if not memory:\n",
    "            prompt = \"Question: \" + question + \" Answer: \"\n",
    "        else : \n",
    "            template = \"Question: {} Answer: {}\"\n",
    "            prompt = \" \".join(\n",
    "                [\n",
    "                    template.format(memory[i][0], memory[i][1])\n",
    "                    for i in range(len(memory))\n",
    "                ]\n",
    "            ) + \" Question: \" + question + \" Answer: \"\n",
    "\n",
    "            # Generate text\n",
    "            inputs = blip_processor(image, text=prompt, return_tensors = \"pt\")\n",
    "            inputs = inputs.to(device, torch.float16)\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens = 100)\n",
    "            generated_text = blip_processor.batch_decode(\n",
    "                generated_ids,\n",
    "                skip_special_tokens = True\n",
    "            )\n",
    "\n",
    "            generated_text = generated_text[0].strip().strip(\"Question\")[0]\n",
    "\n",
    "            # Update Memory\n",
    "            memory.append((question, generated_text))\n",
    "\n",
    "            # Assign to output \n",
    "            output.append_display_data(HTML(\"<b>USER: <b>\" + question))\n",
    "            output.append_display_data(HTML(\"<b>BLIP-2: <b>\"+ generated_text))\n",
    "            output.append_display_data(HTML(\"<b>\"))\n",
    "\n",
    "        # Prepare widgets\n",
    "        in_text = widgets.Text()\n",
    "        in_text.continuous_update = False\n",
    "        in_text.observe(text_eventhandler, \"value\")\n",
    "        output = widgets.Output()\n",
    "        memory = []\n",
    "\n",
    "        # Display chat box\n",
    "        display(\n",
    "            widgets.VBox(\n",
    "                children = [output, in_text],\n",
    "                layout = widgets.Layout(display = \"inline-flex\", flex_flow = \"column-reverse\")\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, AddedToken\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "\n",
    "processor.num_query_tokens = model.config.num_query_tokens\n",
    "image_token = AddedToken(\"<image>\", normalized=False, special=True)\n",
    "processor.tokenizer.add_tokens([image_token], special_tokens=True)\n",
    "\n",
    "model.resize_token_embeddings(len(processor.tokenizer), pad_to_multiple_of=64) # pad for efficient computation\n",
    "model.config.image_token_index = len(processor.tokenizer) - 1\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "prompt = \"Question: how many cats are there? Answer:\"\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HandsOnLLMs",
   "language": "python",
   "name": "handsonllms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
